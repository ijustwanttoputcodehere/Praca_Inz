{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Whiskyfinal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1N3IGeHPawvYYL47zn4-DBMAydsNvKtyp",
      "authorship_tag": "ABX9TyMiQtbdxSOYNHmX8h48uJP3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ijustwanttoputcodehere/Praca_Inz/blob/main/Whiskyfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRhWgPMw8uKX"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ntDe4G9MVv"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/WhiskyDB/scotch_review.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbXZDvWb9Pwi"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uiep4tb9tKB",
        "outputId": "13bf9538-55c2-4457-a376-9002c1fb2016"
      },
      "source": [
        "new_df = df.loc[df['price'] < 200]\n",
        "\n",
        "print(new_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Unnamed: 0  ...                                        description\n",
            "0              1  ...  Magnificently powerful and intense. Caramels, ...\n",
            "1              2  ...  What impresses me most is how this whisky evol...\n",
            "2              3  ...  There have been some legendary Bowmores from t...\n",
            "3              4  ...  With a name inspired by a 1926 Buster Keaton m...\n",
            "4              5  ...  Captivating, enticing, and wonderfully charmin...\n",
            "...          ...  ...                                                ...\n",
            "2242        2243  ...  Its best attributes are vanilla, toasted cocon...\n",
            "2243        2244  ...  Aged in a sherry cask, which adds sweet notes ...\n",
            "2244        2245  ...  Earthy, fleshy notes with brooding grape notes...\n",
            "2245        2246  ...  The sherry is very dominant and cloying, which...\n",
            "2246        2247  ...  Fiery peat kiln smoke, tar, and ripe barley on...\n",
            "\n",
            "[2247 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwk045Ky9bAE"
      },
      "source": [
        "import re\n",
        "pom = new_df.iloc[:,-1:]\n",
        "pom_list = pom[\"description\"].tolist() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFjLYL1lAfVx",
        "outputId": "6cee19af-1a66-4d9f-d76a-2cff8c06ea8c"
      },
      "source": [
        "pom.describe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of                                             description\n",
              "4     Captivating, enticing, and wonderfully charmin...\n",
              "5     Powerful, muscular, well-textured, and invigor...\n",
              "13    A marriage of three different single malts, ag...\n",
              "14    As you’d expect, solid peat is the first thing...\n",
              "16    An essay in balance on both the aroma and pala...\n",
              "...                                                 ...\n",
              "2241  Youthful, and somewhat brooding for a Tomintou...\n",
              "2242  Its best attributes are vanilla, toasted cocon...\n",
              "2243  Aged in a sherry cask, which adds sweet notes ...\n",
              "2244  Earthy, fleshy notes with brooding grape notes...\n",
              "2246  Fiery peat kiln smoke, tar, and ripe barley on...\n",
              "\n",
              "[1659 rows x 1 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij9nWsCH9dR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d5543c-80c2-4e46-96c3-da7cb55481de"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words='english',min_df=4)\n",
        "\n",
        "vectorizer.fit((pom_list))\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['000',\n",
              " '10',\n",
              " '100',\n",
              " '105',\n",
              " '11',\n",
              " '110',\n",
              " '115',\n",
              " '12',\n",
              " '120',\n",
              " '125',\n",
              " '13',\n",
              " '130',\n",
              " '135',\n",
              " '14',\n",
              " '15',\n",
              " '150',\n",
              " '155',\n",
              " '16',\n",
              " '160',\n",
              " '162',\n",
              " '17',\n",
              " '175',\n",
              " '18',\n",
              " '180',\n",
              " '19',\n",
              " '190',\n",
              " '1964',\n",
              " '1966',\n",
              " '1967',\n",
              " '1968',\n",
              " '1969',\n",
              " '1970',\n",
              " '1970s',\n",
              " '1972',\n",
              " '1973',\n",
              " '1974',\n",
              " '1975',\n",
              " '1976',\n",
              " '1977',\n",
              " '1978',\n",
              " '1979',\n",
              " '198',\n",
              " '1980s',\n",
              " '1982',\n",
              " '1983',\n",
              " '1984',\n",
              " '1987',\n",
              " '1988',\n",
              " '1989',\n",
              " '1990',\n",
              " '1990s',\n",
              " '1991',\n",
              " '1992',\n",
              " '1993',\n",
              " '1994',\n",
              " '1995',\n",
              " '1996',\n",
              " '1997',\n",
              " '1998',\n",
              " '1999',\n",
              " '20',\n",
              " '200',\n",
              " '2000',\n",
              " '2001',\n",
              " '2002',\n",
              " '2003',\n",
              " '2004',\n",
              " '2005',\n",
              " '2006',\n",
              " '2007',\n",
              " '2008',\n",
              " '2009',\n",
              " '200th',\n",
              " '2010',\n",
              " '2011',\n",
              " '2012',\n",
              " '2013',\n",
              " '2014',\n",
              " '2015',\n",
              " '2016',\n",
              " '2017',\n",
              " '21',\n",
              " '22',\n",
              " '225',\n",
              " '23',\n",
              " '230',\n",
              " '24',\n",
              " '240',\n",
              " '246',\n",
              " '25',\n",
              " '250',\n",
              " '26',\n",
              " '27',\n",
              " '28',\n",
              " '280',\n",
              " '285',\n",
              " '29',\n",
              " '30',\n",
              " '300',\n",
              " '31',\n",
              " '32',\n",
              " '33',\n",
              " '34',\n",
              " '35',\n",
              " '350',\n",
              " '36',\n",
              " '37',\n",
              " '38',\n",
              " '39',\n",
              " '40',\n",
              " '400',\n",
              " '41',\n",
              " '42',\n",
              " '420',\n",
              " '43',\n",
              " '44',\n",
              " '45',\n",
              " '450',\n",
              " '46',\n",
              " '47',\n",
              " '48',\n",
              " '49',\n",
              " '50',\n",
              " '500',\n",
              " '53',\n",
              " '54',\n",
              " '55',\n",
              " '57',\n",
              " '60',\n",
              " '600',\n",
              " '60s',\n",
              " '62',\n",
              " '63',\n",
              " '64',\n",
              " '65',\n",
              " '68',\n",
              " '69',\n",
              " '70',\n",
              " '700',\n",
              " '74',\n",
              " '75',\n",
              " '750',\n",
              " '80',\n",
              " '800',\n",
              " '85',\n",
              " '850',\n",
              " '87',\n",
              " '88',\n",
              " '89',\n",
              " '90',\n",
              " '90s',\n",
              " '91',\n",
              " '95',\n",
              " '99',\n",
              " 'aberdeenshire',\n",
              " 'aberfeldy',\n",
              " 'aberlour',\n",
              " 'able',\n",
              " 'abv',\n",
              " 'accented',\n",
              " 'accents',\n",
              " 'accentuated',\n",
              " 'accentuates',\n",
              " 'acceptable',\n",
              " 'accompanied',\n",
              " 'accompany',\n",
              " 'accomplished',\n",
              " 'according',\n",
              " 'acetone',\n",
              " 'achieved',\n",
              " 'acidic',\n",
              " 'acidity',\n",
              " 'acquired',\n",
              " 'acrid',\n",
              " 'active',\n",
              " 'actually',\n",
              " 'add',\n",
              " 'added',\n",
              " 'adding',\n",
              " 'addition',\n",
              " 'additional',\n",
              " 'adds',\n",
              " 'adelphi',\n",
              " 'adore',\n",
              " 'affair',\n",
              " 'afternoon',\n",
              " 'agave',\n",
              " 'age',\n",
              " 'aged',\n",
              " 'ages',\n",
              " 'aggressive',\n",
              " 'aging',\n",
              " 'ago',\n",
              " 'agricole',\n",
              " 'ahead',\n",
              " 'aids',\n",
              " 'aim',\n",
              " 'air',\n",
              " 'akin',\n",
              " 'albeit',\n",
              " 'alcohol',\n",
              " 'allow',\n",
              " 'allowing',\n",
              " 'allows',\n",
              " 'allspice',\n",
              " 'almond',\n",
              " 'almonds',\n",
              " 'alongside',\n",
              " 'alternative',\n",
              " 'altogether',\n",
              " 'amazing',\n",
              " 'amazingly',\n",
              " 'amber',\n",
              " 'america',\n",
              " 'american',\n",
              " 'amontillado',\n",
              " 'amoroso',\n",
              " 'amplified',\n",
              " 'anchored',\n",
              " 'ancient',\n",
              " 'angelica',\n",
              " 'anise',\n",
              " 'aniseed',\n",
              " 'anniversary',\n",
              " 'annual',\n",
              " 'answer',\n",
              " 'antique',\n",
              " 'antiseptic',\n",
              " 'anytime',\n",
              " 'apart',\n",
              " 'aperitif',\n",
              " 'apparent',\n",
              " 'apparently',\n",
              " 'appeal',\n",
              " 'appealing',\n",
              " 'appear',\n",
              " 'appearance',\n",
              " 'appeared',\n",
              " 'appears',\n",
              " 'appetizing',\n",
              " 'appetizingly',\n",
              " 'apple',\n",
              " 'apples',\n",
              " 'appreciate',\n",
              " 'approachable',\n",
              " 'approx',\n",
              " 'apricot',\n",
              " 'apricots',\n",
              " 'april',\n",
              " 'ardbeg',\n",
              " 'ardmore',\n",
              " 'area',\n",
              " 'aren',\n",
              " 'armagnac',\n",
              " 'aroma',\n",
              " 'aromas',\n",
              " 'aromatic',\n",
              " 'aromatics',\n",
              " 'arran',\n",
              " 'array',\n",
              " 'arrival',\n",
              " 'art',\n",
              " 'ash',\n",
              " 'ashy',\n",
              " 'asia',\n",
              " 'aside',\n",
              " 'ask',\n",
              " 'aspect',\n",
              " 'aspects',\n",
              " 'asphalt',\n",
              " 'assam',\n",
              " 'assault',\n",
              " 'assertive',\n",
              " 'assorted',\n",
              " 'assured',\n",
              " 'astringency',\n",
              " 'astringent',\n",
              " 'athol',\n",
              " 'attack',\n",
              " 'attention',\n",
              " 'attitude',\n",
              " 'attractive',\n",
              " 'atypical',\n",
              " 'auchentoshan',\n",
              " 'auchroisk',\n",
              " 'aultmore',\n",
              " 'austere',\n",
              " 'autumn',\n",
              " 'availability',\n",
              " 'available',\n",
              " 'avenue',\n",
              " 'away',\n",
              " 'babies',\n",
              " 'backbone',\n",
              " 'backdrop',\n",
              " 'backed',\n",
              " 'background',\n",
              " 'bacon',\n",
              " 'bad',\n",
              " 'baked',\n",
              " 'baker',\n",
              " 'baking',\n",
              " 'balance',\n",
              " 'balanced',\n",
              " 'balances',\n",
              " 'balancing',\n",
              " 'balblair',\n",
              " 'balls',\n",
              " 'balmenach',\n",
              " 'balsamic',\n",
              " 'balvenie',\n",
              " 'banana',\n",
              " 'bananas',\n",
              " 'band',\n",
              " 'banoffee',\n",
              " 'bar',\n",
              " 'barbecue',\n",
              " 'barbecued',\n",
              " 'barely',\n",
              " 'bargain',\n",
              " 'bark',\n",
              " 'barley',\n",
              " 'barnyard',\n",
              " 'barolo',\n",
              " 'barrel',\n",
              " 'barrels',\n",
              " 'barriques',\n",
              " 'bars',\n",
              " 'base',\n",
              " 'based',\n",
              " 'basil',\n",
              " 'batch',\n",
              " 'batches',\n",
              " 'battle',\n",
              " 'bay',\n",
              " 'beach',\n",
              " 'bean',\n",
              " 'beans',\n",
              " 'bears',\n",
              " 'beast',\n",
              " 'beautiful',\n",
              " 'beautifully',\n",
              " 'beauty',\n",
              " 'bed',\n",
              " 'bedding',\n",
              " 'beef',\n",
              " 'beeswax',\n",
              " 'begin',\n",
              " 'beginning',\n",
              " 'begins',\n",
              " 'behave',\n",
              " 'believe',\n",
              " 'belter',\n",
              " 'ben',\n",
              " 'beneath',\n",
              " 'benefited',\n",
              " 'benign',\n",
              " 'benriach',\n",
              " 'benrinnes',\n",
              " 'benromach',\n",
              " 'berried',\n",
              " 'berries',\n",
              " 'berry',\n",
              " 'best',\n",
              " 'better',\n",
              " 'beverage',\n",
              " 'beveridge',\n",
              " 'big',\n",
              " 'bigger',\n",
              " 'biggest',\n",
              " 'binny',\n",
              " 'biscuit',\n",
              " 'biscuits',\n",
              " 'biscuity',\n",
              " 'bit',\n",
              " 'bitter',\n",
              " 'bitterness',\n",
              " 'bittersweet',\n",
              " 'black',\n",
              " 'blackberry',\n",
              " 'blackcurrant',\n",
              " 'bladnoch',\n",
              " 'blair',\n",
              " 'blanc',\n",
              " 'bland',\n",
              " 'blanket',\n",
              " 'blast',\n",
              " 'blend',\n",
              " 'blended',\n",
              " 'blender',\n",
              " 'blending',\n",
              " 'blends',\n",
              " 'blood',\n",
              " 'bloom',\n",
              " 'blossom',\n",
              " 'blossoms',\n",
              " 'blue',\n",
              " 'blueberries',\n",
              " 'blueberry',\n",
              " 'boasts',\n",
              " 'boat',\n",
              " 'bodega',\n",
              " 'bodied',\n",
              " 'body',\n",
              " 'bog',\n",
              " 'boiled',\n",
              " 'bold',\n",
              " 'bolder',\n",
              " 'bomb',\n",
              " 'bonbons',\n",
              " 'bonfire',\n",
              " 'bonus',\n",
              " 'book',\n",
              " 'bordeaux',\n",
              " 'botanicals',\n",
              " 'bottle',\n",
              " 'bottled',\n",
              " 'bottler',\n",
              " 'bottlers',\n",
              " 'bottles',\n",
              " 'bottling',\n",
              " 'bottlings',\n",
              " 'bouquet',\n",
              " 'bourbon',\n",
              " 'boutique',\n",
              " 'bowl',\n",
              " 'bowmore',\n",
              " 'bowmores',\n",
              " 'box',\n",
              " 'boxes',\n",
              " 'boy',\n",
              " 'bracing',\n",
              " 'bracken',\n",
              " 'brackla',\n",
              " 'bramble',\n",
              " 'bramley',\n",
              " 'bran',\n",
              " 'brand',\n",
              " 'brandy',\n",
              " 'brave',\n",
              " 'brazil',\n",
              " 'bread',\n",
              " 'breakfast',\n",
              " 'breath',\n",
              " 'breeze',\n",
              " 'breezy',\n",
              " 'bridge',\n",
              " 'brief',\n",
              " 'briefly',\n",
              " 'bright',\n",
              " 'brighter',\n",
              " 'brilliant',\n",
              " 'brine',\n",
              " 'bring',\n",
              " 'bringing',\n",
              " 'brings',\n",
              " 'brininess',\n",
              " 'briny',\n",
              " 'brisk',\n",
              " 'british',\n",
              " 'brittle',\n",
              " 'broad',\n",
              " 'brooding',\n",
              " 'brora',\n",
              " 'bros',\n",
              " 'brother',\n",
              " 'brothers',\n",
              " 'brought',\n",
              " 'brown',\n",
              " 'browning',\n",
              " 'bruichladdich',\n",
              " 'brulee',\n",
              " 'brûlée',\n",
              " 'bubble',\n",
              " 'bubblegum',\n",
              " 'bucks',\n",
              " 'build',\n",
              " 'building',\n",
              " 'builds',\n",
              " 'built',\n",
              " 'bun',\n",
              " 'bunadh',\n",
              " 'bunch',\n",
              " 'bunna',\n",
              " 'bunnahabhain',\n",
              " 'burlap',\n",
              " 'burn',\n",
              " 'burning',\n",
              " 'burnished',\n",
              " 'burnt',\n",
              " 'burst',\n",
              " 'bursting',\n",
              " 'butt',\n",
              " 'butter',\n",
              " 'buttered',\n",
              " 'butterscotch',\n",
              " 'buttery',\n",
              " 'butts',\n",
              " 'buy',\n",
              " 'buzz',\n",
              " 'byass',\n",
              " 'bòcan',\n",
              " 'cabernet',\n",
              " 'cacao',\n",
              " 'cake',\n",
              " 'calm',\n",
              " 'calming',\n",
              " 'cambus',\n",
              " 'came',\n",
              " 'campbell',\n",
              " 'campbeltown',\n",
              " 'campfire',\n",
              " 'camphor',\n",
              " 'canada',\n",
              " 'candied',\n",
              " 'candies',\n",
              " 'candle',\n",
              " 'candles',\n",
              " 'candy',\n",
              " 'canned',\n",
              " 'cantaloupe',\n",
              " 'caol',\n",
              " 'caramel',\n",
              " 'caramelized',\n",
              " 'caramels',\n",
              " 'cardamom',\n",
              " 'cardhu',\n",
              " 'careful',\n",
              " 'caribbean',\n",
              " 'carnations',\n",
              " 'carries',\n",
              " 'carry',\n",
              " 'carsebridge',\n",
              " 'case',\n",
              " 'cases',\n",
              " 'cask',\n",
              " 'casks',\n",
              " 'cassia',\n",
              " 'catch',\n",
              " 'category',\n",
              " 'cedar',\n",
              " 'celebrate',\n",
              " 'celebrates',\n",
              " 'celery',\n",
              " 'cellar',\n",
              " 'center',\n",
              " 'central',\n",
              " 'century',\n",
              " 'cereal',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'chaff',\n",
              " 'chalky',\n",
              " 'challenge',\n",
              " 'challenging',\n",
              " 'chamomile',\n",
              " 'chance',\n",
              " 'change',\n",
              " 'changed',\n",
              " 'changes',\n",
              " 'changing',\n",
              " 'chapter',\n",
              " 'char',\n",
              " 'character',\n",
              " 'characterful',\n",
              " 'characteristic',\n",
              " 'characteristics',\n",
              " 'charcoal',\n",
              " 'charcuterie',\n",
              " 'charge',\n",
              " 'charlotte',\n",
              " 'charmer',\n",
              " 'charming',\n",
              " 'charred',\n",
              " 'chateau',\n",
              " 'check',\n",
              " 'cheese',\n",
              " 'cheesecake',\n",
              " 'cherries',\n",
              " 'cherry',\n",
              " 'chestnut',\n",
              " 'chestnuts',\n",
              " 'chests',\n",
              " 'chewing',\n",
              " 'chews',\n",
              " 'chewy',\n",
              " 'chicago',\n",
              " 'chicory',\n",
              " 'chili',\n",
              " 'chill',\n",
              " 'chilled',\n",
              " 'chimney',\n",
              " 'chips',\n",
              " 'chivas',\n",
              " 'chock',\n",
              " 'chocolate',\n",
              " 'choice',\n",
              " 'chopped',\n",
              " 'chosen',\n",
              " 'christmas',\n",
              " 'church',\n",
              " 'cider',\n",
              " 'cigar',\n",
              " 'cigarette',\n",
              " 'cilantro',\n",
              " 'cinnamon',\n",
              " 'circulation',\n",
              " 'citric',\n",
              " 'citrus',\n",
              " 'class',\n",
              " 'classic',\n",
              " 'classy',\n",
              " 'clean',\n",
              " 'cleaner',\n",
              " 'clear',\n",
              " 'clearly',\n",
              " 'clementine',\n",
              " 'clinging',\n",
              " 'clingy',\n",
              " 'close',\n",
              " 'closed',\n",
              " 'closer',\n",
              " 'clotted',\n",
              " 'clouds',\n",
              " 'cloudy',\n",
              " 'clove',\n",
              " 'clover',\n",
              " 'cloves',\n",
              " 'cloying',\n",
              " 'club',\n",
              " 'clynelish',\n",
              " 'coal',\n",
              " 'coast',\n",
              " 'coastal',\n",
              " 'coated',\n",
              " 'coating',\n",
              " 'coats',\n",
              " 'cobbler',\n",
              " 'cocktail',\n",
              " 'cocoa',\n",
              " 'coconut',\n",
              " 'coffee',\n",
              " 'cognac',\n",
              " 'cola',\n",
              " 'cold',\n",
              " 'collection',\n",
              " 'color',\n",
              " 'colored',\n",
              " 'com',\n",
              " 'combination',\n",
              " 'combine',\n",
              " 'combined',\n",
              " 'combines',\n",
              " 'combining',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'comfort',\n",
              " 'comforting',\n",
              " 'coming',\n",
              " 'comment',\n",
              " 'common',\n",
              " 'company',\n",
              " 'compared',\n",
              " 'comparison',\n",
              " 'compass',\n",
              " 'competition',\n",
              " 'complement',\n",
              " 'complementary',\n",
              " 'complemented',\n",
              " 'complete',\n",
              " 'completely',\n",
              " 'complex',\n",
              " 'complexities',\n",
              " 'complexity',\n",
              " 'component',\n",
              " 'components',\n",
              " 'composition',\n",
              " 'compote',\n",
              " 'comprises',\n",
              " 'comprising',\n",
              " 'concentrated',\n",
              " 'concentration',\n",
              " 'concept',\n",
              " 'concerned',\n",
              " 'conclusion',\n",
              " 'concoction',\n",
              " 'condensed',\n",
              " 'confectioner',\n",
              " 'confident',\n",
              " 'confirms',\n",
              " 'considerable',\n",
              " 'considered',\n",
              " 'considering',\n",
              " 'consistent',\n",
              " 'consists',\n",
              " 'constantly',\n",
              " 'contained',\n",
              " 'containing',\n",
              " 'contains',\n",
              " 'contemplative',\n",
              " 'content',\n",
              " 'continue',\n",
              " 'continued',\n",
              " 'continues',\n",
              " 'contrast',\n",
              " 'contrasting',\n",
              " 'contribute',\n",
              " 'contributes',\n",
              " 'control',\n",
              " 'cooked',\n",
              " 'cookie',\n",
              " 'cookies',\n",
              " 'cooking',\n",
              " 'cool',\n",
              " 'copper',\n",
              " 'cordial',\n",
              " 'core',\n",
              " 'coriander',\n",
              " 'corn',\n",
              " 'cornflakes',\n",
              " 'cost',\n",
              " 'cotton',\n",
              " 'cough',\n",
              " 'counterpoint',\n",
              " 'country',\n",
              " 'couple',\n",
              " 'course',\n",
              " 'covered',\n",
              " 'cracked',\n",
              " 'cracker',\n",
              " 'crackers',\n",
              " 'cragganmore',\n",
              " 'craigellachie',\n",
              " 'cranberries',\n",
              " 'cranberry',\n",
              " 'cream',\n",
              " 'creamier',\n",
              " 'creaminess',\n",
              " 'creams',\n",
              " 'creamsicle',\n",
              " 'creamy',\n",
              " 'create',\n",
              " 'created',\n",
              " 'creation',\n",
              " 'creeps',\n",
              " 'creosote',\n",
              " 'crescendo',\n",
              " 'crisp',\n",
              " 'criticism',\n",
              " 'crumble',\n",
              " 'crushed',\n",
              " 'crystallized',\n",
              " 'crème',\n",
              " 'cubes',\n",
              " 'cucumber',\n",
              " 'cult',\n",
              " 'cumin',\n",
              " 'curd',\n",
              " 'curiously',\n",
              " 'currant',\n",
              " 'currants',\n",
              " 'current',\n",
              " 'currently',\n",
              " 'custard',\n",
              " 'cut',\n",
              " 'cuts',\n",
              " 'cutting',\n",
              " 'cutty',\n",
              " 'cù',\n",
              " 'dabs',\n",
              " 'dailuaine',\n",
              " 'dalmore',\n",
              " 'dalwhinnie',\n",
              " 'damp',\n",
              " 'damson',\n",
              " 'damsons',\n",
              " 'dances',\n",
              " 'darjeeling',\n",
              " 'dark',\n",
              " 'darkens',\n",
              " 'darker',\n",
              " 'darkest',\n",
              " 'darkness',\n",
              " 'dash',\n",
              " 'date',\n",
              " 'dates',\n",
              " 'day',\n",
              " 'days',\n",
              " 'dcl',\n",
              " 'dead',\n",
              " 'deanston',\n",
              " 'debut',\n",
              " 'decade',\n",
              " 'decadent',\n",
              " 'decades',\n",
              " 'decanters',\n",
              " 'december',\n",
              " 'decent',\n",
              " 'decidedly',\n",
              " 'deep',\n",
              " 'deeper',\n",
              " 'deeply',\n",
              " 'defined',\n",
              " 'definitely',\n",
              " 'deft',\n",
              " 'degree',\n",
              " 'deliberately',\n",
              " 'delicate',\n",
              " 'delicately',\n",
              " 'delicious',\n",
              " 'deliciously',\n",
              " 'delight',\n",
              " 'delightful',\n",
              " 'delights',\n",
              " 'deliver',\n",
              " 'delivered',\n",
              " 'delivering',\n",
              " 'delivers',\n",
              " 'delivery',\n",
              " 'demeanor',\n",
              " 'demerara',\n",
              " 'demolished',\n",
              " 'demonstrates',\n",
              " 'dense',\n",
              " 'density',\n",
              " 'departure',\n",
              " 'depends',\n",
              " 'depot',\n",
              " 'depth',\n",
              " 'depths',\n",
              " 'derived',\n",
              " 'described',\n",
              " 'despite',\n",
              " 'dessert',\n",
              " 'develop',\n",
              " 'developing',\n",
              " 'development',\n",
              " 'develops',\n",
              " 'devil',\n",
              " 'devotees',\n",
              " 'dewar',\n",
              " 'diageo',\n",
              " 'did',\n",
              " 'difference',\n",
              " 'different',\n",
              " 'difficult',\n",
              " 'digestive',\n",
              " 'dilute',\n",
              " 'diluted',\n",
              " 'dilution',\n",
              " 'dimension',\n",
              " 'dimensional',\n",
              " 'dinner',\n",
              " 'dipped',\n",
              " 'direct',\n",
              " 'direction',\n",
              " 'disappoint',\n",
              " 'disappointed',\n",
              " 'discreet',\n",
              " 'discreetly',\n",
              " 'displays',\n",
              " 'distant',\n",
              " 'distillate',\n",
              " 'distillation',\n",
              " 'distilled',\n",
              " 'distiller',\n",
              " 'distilleries',\n",
              " 'distillers',\n",
              " 'distillery',\n",
              " 'distinct',\n",
              " 'distinctive',\n",
              " 'distinctly',\n",
              " 'dna',\n",
              " 'docks',\n",
              " 'does',\n",
              " 'doesn',\n",
              " 'dog',\n",
              " 'dollop',\n",
              " 'dominant',\n",
              " 'dominate',\n",
              " 'dominated',\n",
              " 'dominates',\n",
              " 'dominating',\n",
              " 'don',\n",
              " 'dose',\n",
              " 'double',\n",
              " 'doubt',\n",
              " 'dough',\n",
              " 'douglas',\n",
              " 'dovetail',\n",
              " 'dovetails',\n",
              " 'dozen',\n",
              " 'draff',\n",
              " 'dram',\n",
              " 'drams',\n",
              " 'drawing',\n",
              " 'drawn',\n",
              " 'drenched',\n",
              " 'dried',\n",
              " 'drier',\n",
              " 'dries',\n",
              " 'drifts',\n",
              " 'driftwood',\n",
              " 'drink',\n",
              " 'drinkability',\n",
              " 'drinkable',\n",
              " 'drinker',\n",
              " 'drinkers',\n",
              " 'drinking',\n",
              " 'drinks',\n",
              " 'dripping',\n",
              " 'drive',\n",
              " 'driven',\n",
              " 'drizzle',\n",
              " 'drizzled',\n",
              " 'drop',\n",
              " 'drops',\n",
              " 'dry',\n",
              " 'drying',\n",
              " 'dryish',\n",
              " 'dryness',\n",
              " 'dufftown',\n",
              " 'dug',\n",
              " 'dull',\n",
              " 'duncan',\n",
              " 'dundas',\n",
              " 'dundee',\n",
              " 'dunnage',\n",
              " 'dusted',\n",
              " 'dusting',\n",
              " 'dusty',\n",
              " 'duty',\n",
              " 'dynamic',\n",
              " 'earl',\n",
              " 'earlier',\n",
              " 'early',\n",
              " 'earth',\n",
              " 'earthiness',\n",
              " 'earthy',\n",
              " 'easily',\n",
              " 'easy',\n",
              " 'eating',\n",
              " 'echo',\n",
              " 'edge',\n",
              " 'edges',\n",
              " 'edgy',\n",
              " 'edinburgh',\n",
              " 'edition',\n",
              " 'editions',\n",
              " 'editor',\n",
              " 'edradour',\n",
              " 'effect',\n",
              " 'effort',\n",
              " 'efforts',\n",
              " 'elder',\n",
              " 'elderflower',\n",
              " 'elegance',\n",
              " 'elegant',\n",
              " 'elegantly',\n",
              " 'element',\n",
              " 'elements',\n",
              " 'elgin',\n",
              " 'ellen',\n",
              " 'elusive',\n",
              " 'ember',\n",
              " 'embers',\n",
              " 'embrace',\n",
              " 'embraces',\n",
              " 'emerge',\n",
              " 'emerges',\n",
              " 'emerging',\n",
              " 'end',\n",
              " 'ending',\n",
              " 'ends',\n",
              " 'enduring',\n",
              " 'energy',\n",
              " 'engine',\n",
              " 'english',\n",
              " 'enhance',\n",
              " 'enhanced',\n",
              " 'enhances',\n",
              " 'enjoy',\n",
              " 'enjoyable',\n",
              " 'enjoyed',\n",
              " 'entertaining',\n",
              " 'enthusiasts',\n",
              " 'enticing',\n",
              " 'entire',\n",
              " 'entirely',\n",
              " 'entry',\n",
              " 'enveloping',\n",
              " 'envelops',\n",
              " 'equal',\n",
              " 'equally',\n",
              " 'especially',\n",
              " 'espresso',\n",
              " 'esque',\n",
              " 'essence',\n",
              " 'established',\n",
              " 'estates',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG5_6SSg9fxd"
      },
      "source": [
        "pomy = df['review.point']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIw-oIPp9mET"
      },
      "source": [
        "import re\n",
        "\n",
        "int_price=[]\n",
        "\n",
        "# finding out the complications in the column\n",
        "for i in df['price']:\n",
        "    #removing $ and , so that we can convert this feature price into integer type\n",
        "    _=re.sub(r'[$,]+','',i)\n",
        "    #converting float integrs\n",
        "    x=re.sub(r'\\W\\d\\d','',_)\n",
        "    z=0\n",
        "    #converting liter into one botle price\n",
        "    if (\"/l\" in x):\n",
        "        l=re.sub(r'[/l]\\w+','',x)\n",
        "        \n",
        "        z=int(l)\n",
        "        z=z*.75\n",
        "        int_price.append(z)\n",
        "        \n",
        "    # if any of the alphanumeric value like space like we encountered the case : ('$15,000 or $60,000/set')     \n",
        "    elif(\" \" in  x):\n",
        "        l= re.sub(r'[ ]\\w+\\W+\\w+','',x)\n",
        "        z=int(l)\n",
        "        int_price.append(z)\n",
        "            \n",
        "    elif (\"set\" in x):\n",
        "        l=re.sub(r'[/]\\w+','',x)\n",
        "        \n",
        "        z= int(l)\n",
        "        z=z/4\n",
        "        int_price.append(z)\n",
        "    else :\n",
        "        z=int(x)\n",
        "        int_price.append(z)\n",
        "print(int_price)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XXJy7Zp9pCZ"
      },
      "source": [
        "df['price']=int_price\n",
        "df['price']=df['price'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXr7QpWoBwM4",
        "outputId": "4e7a6ff2-1ff4-4053-b17b-d6dbbbf7ec53"
      },
      "source": [
        "new_df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0       int64\n",
              "name            object\n",
              "category        object\n",
              "review.point     int64\n",
              "price            int64\n",
              "currency        object\n",
              "description     object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUNC287G-FAb"
      },
      "source": [
        "vector = vectorizer.transform(pom_list)\n",
        "vector_spaces = vector.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqWPKcVKB68M",
        "outputId": "4bee2219-c519-4d9c-d6ac-429bb52b98db"
      },
      "source": [
        "vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1659x2388 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 58935 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdeIm6GhCBVS"
      },
      "source": [
        "X = np.array(vector_spaces)\n",
        "#y = np.array(pomy)\n",
        "y = np.array(new_df['price'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRF0OczfDHWw",
        "outputId": "3e6f6092-6ab4-4cf7-a34c-6b20709c17f9"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt.fit(X, y)\n",
        "\n",
        "predictions = gbrt.predict(X_test)\n",
        "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 31.397653240417153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Odljd24YE8w_",
        "outputId": "a4390f3e-fea1-4502-9b5f-3a4258601f83"
      },
      "source": [
        "new_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review.point</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1659.000000</td>\n",
              "      <td>1659.000000</td>\n",
              "      <td>1659.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1259.715491</td>\n",
              "      <td>85.890898</td>\n",
              "      <td>92.770344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>602.349720</td>\n",
              "      <td>3.714417</td>\n",
              "      <td>40.891863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>773.500000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1307.000000</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>85.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1777.500000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>120.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2247.000000</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>199.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  review.point        price\n",
              "count  1659.000000   1659.000000  1659.000000\n",
              "mean   1259.715491     85.890898    92.770344\n",
              "std     602.349720      3.714417    40.891863\n",
              "min       5.000000     63.000000    12.000000\n",
              "25%     773.500000     84.000000    61.000000\n",
              "50%    1307.000000     86.000000    85.000000\n",
              "75%    1777.500000     88.000000   120.500000\n",
              "max    2247.000000     96.000000   199.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfa9xkirIBQK",
        "outputId": "b40bf0e2-7e7a-4e7c-ee3b-291311aa61a5"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "neigh = KNeighborsRegressor(n_neighbors=2,metric='manhattan')\n",
        "neigh.fit(X,y)\n",
        "\n",
        "predictions = neigh.predict(X_test)\n",
        "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 43.456626506024094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4-ZY9pkIkro"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
        "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(120, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(240, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(120, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8jB79vbJzlU",
        "outputId": "9c893d99-f2e7-4cc6-a918-0b5f056e31a7"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 43, 101, 120, ...,  60,  55, 132])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbJO2wceL5ZS"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "# applying the scaling to the test set that we computed for the training set\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVZUOthKJ7Dx",
        "outputId": "b689292c-800e-4f1d-f5a4-9f4fe4f8e04f"
      },
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\",metrics=['mae'])\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "39/39 [==============================] - 1s 9ms/step - loss: 2.5580 - mae: 3.0184 - val_loss: 32.7402 - val_mae: 33.2386\n",
            "Epoch 2/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 2.7391 - mae: 3.2080 - val_loss: 32.2308 - val_mae: 32.7297\n",
            "Epoch 3/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.4183 - mae: 1.8434 - val_loss: 32.3862 - val_mae: 32.8826\n",
            "Epoch 4/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.7024 - mae: 2.1510 - val_loss: 32.4120 - val_mae: 32.9089\n",
            "Epoch 5/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5754 - mae: 2.0192 - val_loss: 33.0620 - val_mae: 33.5575\n",
            "Epoch 6/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.8133 - mae: 2.2601 - val_loss: 32.5975 - val_mae: 33.0949\n",
            "Epoch 7/100\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 1.6925 - mae: 2.1354 - val_loss: 32.2710 - val_mae: 32.7691\n",
            "Epoch 8/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8270 - mae: 2.2742 - val_loss: 32.2160 - val_mae: 32.7147\n",
            "Epoch 9/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8678 - mae: 2.3159 - val_loss: 32.5299 - val_mae: 33.0262\n",
            "Epoch 10/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.7137 - mae: 2.1630 - val_loss: 32.1117 - val_mae: 32.6090\n",
            "Epoch 11/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.7746 - mae: 2.2194 - val_loss: 33.0293 - val_mae: 33.5238\n",
            "Epoch 12/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8930 - mae: 2.3403 - val_loss: 32.5313 - val_mae: 33.0287\n",
            "Epoch 13/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8247 - mae: 2.2701 - val_loss: 32.6859 - val_mae: 33.1836\n",
            "Epoch 14/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8467 - mae: 2.2961 - val_loss: 32.2519 - val_mae: 32.7482\n",
            "Epoch 15/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 2.0724 - mae: 2.5285 - val_loss: 33.4191 - val_mae: 33.9142\n",
            "Epoch 16/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 2.2264 - mae: 2.6849 - val_loss: 32.1964 - val_mae: 32.6938\n",
            "Epoch 17/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8472 - mae: 2.3032 - val_loss: 32.6789 - val_mae: 33.1765\n",
            "Epoch 18/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8289 - mae: 2.2812 - val_loss: 32.7644 - val_mae: 33.2606\n",
            "Epoch 19/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.9110 - mae: 2.3624 - val_loss: 32.0060 - val_mae: 32.5032\n",
            "Epoch 20/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6430 - mae: 2.0862 - val_loss: 32.6348 - val_mae: 33.1322\n",
            "Epoch 21/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.7381 - mae: 2.1859 - val_loss: 32.7291 - val_mae: 33.2256\n",
            "Epoch 22/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.8674 - mae: 2.3187 - val_loss: 32.2428 - val_mae: 32.7413\n",
            "Epoch 23/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8197 - mae: 2.2667 - val_loss: 32.2679 - val_mae: 32.7650\n",
            "Epoch 24/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5458 - mae: 1.9801 - val_loss: 32.3972 - val_mae: 32.8961\n",
            "Epoch 25/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.6735 - mae: 2.1114 - val_loss: 32.4995 - val_mae: 32.9965\n",
            "Epoch 26/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 2.5009 - mae: 2.9607 - val_loss: 32.9655 - val_mae: 33.4625\n",
            "Epoch 27/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 2.2724 - mae: 2.7262 - val_loss: 32.5899 - val_mae: 33.0855\n",
            "Epoch 28/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6169 - mae: 2.0573 - val_loss: 32.3831 - val_mae: 32.8816\n",
            "Epoch 29/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.4510 - mae: 1.8885 - val_loss: 32.7421 - val_mae: 33.2392\n",
            "Epoch 30/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.4304 - mae: 1.8658 - val_loss: 32.6890 - val_mae: 33.1879\n",
            "Epoch 31/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5414 - mae: 1.9811 - val_loss: 32.0752 - val_mae: 32.5746\n",
            "Epoch 32/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.7529 - mae: 2.1993 - val_loss: 32.6706 - val_mae: 33.1699\n",
            "Epoch 33/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.6902 - mae: 2.1359 - val_loss: 32.4336 - val_mae: 32.9311\n",
            "Epoch 34/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 2.0470 - mae: 2.4979 - val_loss: 32.4910 - val_mae: 32.9892\n",
            "Epoch 35/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8272 - mae: 2.2718 - val_loss: 32.4697 - val_mae: 32.9696\n",
            "Epoch 36/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6821 - mae: 2.1260 - val_loss: 32.3215 - val_mae: 32.8172\n",
            "Epoch 37/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5256 - mae: 1.9643 - val_loss: 32.3010 - val_mae: 32.7989\n",
            "Epoch 38/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.6368 - mae: 2.0737 - val_loss: 32.4468 - val_mae: 32.9445\n",
            "Epoch 39/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.6641 - mae: 2.1116 - val_loss: 32.2111 - val_mae: 32.7096\n",
            "Epoch 40/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.4475 - mae: 1.8822 - val_loss: 32.6759 - val_mae: 33.1729\n",
            "Epoch 41/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6203 - mae: 2.0601 - val_loss: 32.1215 - val_mae: 32.6197\n",
            "Epoch 42/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6621 - mae: 2.1032 - val_loss: 32.8707 - val_mae: 33.3684\n",
            "Epoch 43/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.9027 - mae: 2.3477 - val_loss: 32.0399 - val_mae: 32.5366\n",
            "Epoch 44/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.8115 - mae: 2.2606 - val_loss: 32.5757 - val_mae: 33.0740\n",
            "Epoch 45/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.6864 - mae: 2.1283 - val_loss: 32.7925 - val_mae: 33.2905\n",
            "Epoch 46/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 2.2595 - mae: 2.7151 - val_loss: 32.2315 - val_mae: 32.7287\n",
            "Epoch 47/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.8636 - mae: 2.3114 - val_loss: 32.2426 - val_mae: 32.7409\n",
            "Epoch 48/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5316 - mae: 1.9706 - val_loss: 32.6913 - val_mae: 33.1886\n",
            "Epoch 49/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6026 - mae: 2.0457 - val_loss: 32.0971 - val_mae: 32.5946\n",
            "Epoch 50/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6726 - mae: 2.1151 - val_loss: 32.5576 - val_mae: 33.0536\n",
            "Epoch 51/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5926 - mae: 2.0341 - val_loss: 32.4217 - val_mae: 32.9181\n",
            "Epoch 52/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5396 - mae: 1.9769 - val_loss: 32.1590 - val_mae: 32.6546\n",
            "Epoch 53/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5777 - mae: 2.0161 - val_loss: 32.1993 - val_mae: 32.6967\n",
            "Epoch 54/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3147 - mae: 1.7422 - val_loss: 32.2653 - val_mae: 32.7624\n",
            "Epoch 55/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3414 - mae: 1.7655 - val_loss: 31.9278 - val_mae: 32.4261\n",
            "Epoch 56/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5769 - mae: 2.0160 - val_loss: 32.7929 - val_mae: 33.2915\n",
            "Epoch 57/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.5782 - mae: 2.0158 - val_loss: 32.1661 - val_mae: 32.6639\n",
            "Epoch 58/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.3544 - mae: 1.7818 - val_loss: 31.9263 - val_mae: 32.4237\n",
            "Epoch 59/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.1859 - mae: 1.6098 - val_loss: 32.1616 - val_mae: 32.6596\n",
            "Epoch 60/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3885 - mae: 1.8148 - val_loss: 32.6324 - val_mae: 33.1301\n",
            "Epoch 61/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.4661 - mae: 1.9056 - val_loss: 32.2869 - val_mae: 32.7840\n",
            "Epoch 62/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.4941 - mae: 1.9322 - val_loss: 32.4709 - val_mae: 32.9655\n",
            "Epoch 63/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2817 - mae: 1.7080 - val_loss: 32.3368 - val_mae: 32.8335\n",
            "Epoch 64/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2091 - mae: 1.6329 - val_loss: 32.1039 - val_mae: 32.6008\n",
            "Epoch 65/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.2971 - mae: 1.7253 - val_loss: 32.1728 - val_mae: 32.6712\n",
            "Epoch 66/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.5727 - mae: 2.0086 - val_loss: 32.4412 - val_mae: 32.9380\n",
            "Epoch 67/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3217 - mae: 1.7488 - val_loss: 32.3078 - val_mae: 32.8026\n",
            "Epoch 68/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.3265 - mae: 1.7592 - val_loss: 32.2438 - val_mae: 32.7406\n",
            "Epoch 69/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2997 - mae: 1.7265 - val_loss: 32.4201 - val_mae: 32.9162\n",
            "Epoch 70/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3298 - mae: 1.7543 - val_loss: 32.3491 - val_mae: 32.8461\n",
            "Epoch 71/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.4326 - mae: 1.8712 - val_loss: 32.2109 - val_mae: 32.7075\n",
            "Epoch 72/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.4061 - mae: 1.8421 - val_loss: 32.6636 - val_mae: 33.1601\n",
            "Epoch 73/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.4335 - mae: 1.8667 - val_loss: 32.3909 - val_mae: 32.8855\n",
            "Epoch 74/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.1387 - mae: 1.5604 - val_loss: 32.4224 - val_mae: 32.9196\n",
            "Epoch 75/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.1998 - mae: 1.6254 - val_loss: 32.2823 - val_mae: 32.7787\n",
            "Epoch 76/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2396 - mae: 1.6654 - val_loss: 32.3624 - val_mae: 32.8588\n",
            "Epoch 77/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.2943 - mae: 1.7173 - val_loss: 31.8353 - val_mae: 32.3320\n",
            "Epoch 78/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3988 - mae: 1.8356 - val_loss: 32.2731 - val_mae: 32.7713\n",
            "Epoch 79/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2672 - mae: 1.6929 - val_loss: 32.3624 - val_mae: 32.8577\n",
            "Epoch 80/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.1818 - mae: 1.6114 - val_loss: 32.5552 - val_mae: 33.0522\n",
            "Epoch 81/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.3205 - mae: 1.7508 - val_loss: 32.1116 - val_mae: 32.6059\n",
            "Epoch 82/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2712 - mae: 1.6965 - val_loss: 32.4888 - val_mae: 32.9837\n",
            "Epoch 83/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.2810 - mae: 1.7099 - val_loss: 32.5343 - val_mae: 33.0299\n",
            "Epoch 84/100\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 1.2495 - mae: 1.6720 - val_loss: 32.0972 - val_mae: 32.5949\n",
            "Epoch 85/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2392 - mae: 1.6623 - val_loss: 32.3886 - val_mae: 32.8854\n",
            "Epoch 86/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.2589 - mae: 1.6827 - val_loss: 32.5171 - val_mae: 33.0141\n",
            "Epoch 87/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.0177 - mae: 1.4246 - val_loss: 32.2401 - val_mae: 32.7367\n",
            "Epoch 88/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.4022 - mae: 1.8394 - val_loss: 32.5845 - val_mae: 33.0778\n",
            "Epoch 89/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3625 - mae: 1.7921 - val_loss: 32.1990 - val_mae: 32.6966\n",
            "Epoch 90/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.1614 - mae: 1.5782 - val_loss: 32.1647 - val_mae: 32.6626\n",
            "Epoch 91/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.1039 - mae: 1.5182 - val_loss: 32.3053 - val_mae: 32.8001\n",
            "Epoch 92/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.1485 - mae: 1.5636 - val_loss: 32.1296 - val_mae: 32.6243\n",
            "Epoch 93/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.1876 - mae: 1.6152 - val_loss: 32.0590 - val_mae: 32.5562\n",
            "Epoch 94/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.2027 - mae: 1.6289 - val_loss: 32.4823 - val_mae: 32.9766\n",
            "Epoch 95/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.4637 - mae: 1.8981 - val_loss: 32.2165 - val_mae: 32.7130\n",
            "Epoch 96/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6661 - mae: 2.1059 - val_loss: 32.4678 - val_mae: 32.9637\n",
            "Epoch 97/100\n",
            "39/39 [==============================] - 0s 4ms/step - loss: 1.6859 - mae: 2.1277 - val_loss: 32.0330 - val_mae: 32.5294\n",
            "Epoch 98/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3320 - mae: 1.7634 - val_loss: 31.9880 - val_mae: 32.4854\n",
            "Epoch 99/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3904 - mae: 1.8174 - val_loss: 32.0489 - val_mae: 32.5467\n",
            "Epoch 100/100\n",
            "39/39 [==============================] - 0s 5ms/step - loss: 1.3871 - mae: 1.8154 - val_loss: 32.1523 - val_mae: 32.6475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LubyWWTKXAc",
        "outputId": "5431d384-a12c-480e-8ea8-2a2b87c6ad3c"
      },
      "source": [
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 2ms/step - loss: 32.1523 - mae: 32.6475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swd8y3qIR5WE"
      },
      "source": [
        "# **Regresja dla oceny**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQEXfY2nQO9A",
        "outputId": "74de90be-881f-4ea4-c100-2cd8ae05f414"
      },
      "source": [
        "pom_df = df.loc[df['review.point'] > 80]\n",
        "pom_df = df\n",
        "print(pom_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Unnamed: 0  ...                                        description\n",
            "0              1  ...  Magnificently powerful and intense. Caramels, ...\n",
            "1              2  ...  What impresses me most is how this whisky evol...\n",
            "2              3  ...  There have been some legendary Bowmores from t...\n",
            "3              4  ...  With a name inspired by a 1926 Buster Keaton m...\n",
            "4              5  ...  Captivating, enticing, and wonderfully charmin...\n",
            "...          ...  ...                                                ...\n",
            "2242        2243  ...  Its best attributes are vanilla, toasted cocon...\n",
            "2243        2244  ...  Aged in a sherry cask, which adds sweet notes ...\n",
            "2244        2245  ...  Earthy, fleshy notes with brooding grape notes...\n",
            "2245        2246  ...  The sherry is very dominant and cloying, which...\n",
            "2246        2247  ...  Fiery peat kiln smoke, tar, and ripe barley on...\n",
            "\n",
            "[2247 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "QT1sMiuQQTkc",
        "outputId": "b087c9b4-485c-4af1-96c6-8e1786d6179e"
      },
      "source": [
        "pom_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review.point</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2247.000000</td>\n",
              "      <td>2247.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1124.000000</td>\n",
              "      <td>86.700045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>648.797349</td>\n",
              "      <td>4.054055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>63.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>562.500000</td>\n",
              "      <td>84.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1124.000000</td>\n",
              "      <td>87.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1685.500000</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2247.000000</td>\n",
              "      <td>97.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  review.point\n",
              "count  2247.000000   2247.000000\n",
              "mean   1124.000000     86.700045\n",
              "std     648.797349      4.054055\n",
              "min       1.000000     63.000000\n",
              "25%     562.500000     84.000000\n",
              "50%    1124.000000     87.000000\n",
              "75%    1685.500000     90.000000\n",
              "max    2247.000000     97.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fBjLxDWQs6e"
      },
      "source": [
        "import re\n",
        "pom1 = pom_df.iloc[:,-1:]\n",
        "pom1_list = pom1[\"description\"].tolist() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4MRknqsQ68m"
      },
      "source": [
        "vectorizer1 = CountVectorizer(stop_words='english',min_df=4)\n",
        "\n",
        "vectorizer1.fit((pom1_list))\n",
        "\n",
        "vectorizer1.get_feature_names()\n",
        "\n",
        "vector1 = vectorizer1.transform(pom1_list)\n",
        "vector_spaces1 = vector1.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF0ha3FERB4O"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X1 = np.array(vector_spaces1)\n",
        "y1 = np.array(pom_df['review.point'])\n",
        "\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk8iPCJPSjCG",
        "outputId": "a03e43ba-af56-4aba-eab5-e050f90a6578"
      },
      "source": [
        "X1_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(562, 2961)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToKYAkofqvoU",
        "outputId": "418c9a48-7f97-4748-c978-1d2d1920f597"
      },
      "source": [
        "X1_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1685, 2961)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6NetS2aSmwd",
        "outputId": "aa458138-b227-48c0-c64e-1473b9751cb6"
      },
      "source": [
        "y1_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(562,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU_8qbqfROSy"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "keras.layers.Dense(30, activation=\"relu\", input_shape=X1_train.shape[1:]),\n",
        "keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFnEbKOGyXyL",
        "outputId": "6a2aebe7-4d53-41b3-e04f-c1ca79edeff4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 30)                88860     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 88,891\n",
            "Trainable params: 88,891\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km1ZRf7-nt63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78beb585-0d8a-4ad3-e699-291a260245ba"
      },
      "source": [
        "model.compile(loss=\"mean_squared_error\", optimizer=\"Adam\",metrics=['mae'])\n",
        "history = model.fit(X1_train, y1_train, epochs=100,\n",
        "validation_data=(X1_test, y1_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "53/53 [==============================] - 1s 5ms/step - loss: 7053.8643 - mae: 83.8804 - val_loss: 6457.9336 - val_mae: 80.2543\n",
            "Epoch 2/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 5737.0366 - mae: 75.5624 - val_loss: 4919.0342 - val_mae: 69.9293\n",
            "Epoch 3/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 4107.2539 - mae: 63.6191 - val_loss: 3242.0732 - val_mae: 56.3456\n",
            "Epoch 4/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 2533.5938 - mae: 49.2592 - val_loss: 1830.3534 - val_mae: 41.2956\n",
            "Epoch 5/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1369.1986 - mae: 34.8532 - val_loss: 959.0275 - val_mae: 28.4598\n",
            "Epoch 6/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 740.4164 - mae: 23.7510 - val_loss: 583.3447 - val_mae: 20.3303\n",
            "Epoch 7/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 493.1596 - mae: 18.0749 - val_loss: 465.8470 - val_mae: 16.8833\n",
            "Epoch 8/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 414.1068 - mae: 15.8360 - val_loss: 432.7680 - val_mae: 15.5818\n",
            "Epoch 9/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 380.8105 - mae: 14.9555 - val_loss: 416.8990 - val_mae: 15.0836\n",
            "Epoch 10/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 358.3566 - mae: 14.3950 - val_loss: 403.6453 - val_mae: 14.6459\n",
            "Epoch 11/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 338.7646 - mae: 13.9316 - val_loss: 391.4062 - val_mae: 14.3256\n",
            "Epoch 12/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 320.9521 - mae: 13.4972 - val_loss: 382.4502 - val_mae: 14.0554\n",
            "Epoch 13/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 305.2043 - mae: 13.1371 - val_loss: 371.7540 - val_mae: 13.8164\n",
            "Epoch 14/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 290.0142 - mae: 12.7920 - val_loss: 363.4142 - val_mae: 13.5916\n",
            "Epoch 15/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 276.4645 - mae: 12.4803 - val_loss: 356.5644 - val_mae: 13.3930\n",
            "Epoch 16/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 264.2352 - mae: 12.1862 - val_loss: 350.7372 - val_mae: 13.2332\n",
            "Epoch 17/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 252.4919 - mae: 11.9361 - val_loss: 343.1395 - val_mae: 13.0808\n",
            "Epoch 18/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 241.4678 - mae: 11.6654 - val_loss: 338.9538 - val_mae: 12.9435\n",
            "Epoch 19/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 231.4342 - mae: 11.4225 - val_loss: 333.6773 - val_mae: 12.8464\n",
            "Epoch 20/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 221.9315 - mae: 11.1983 - val_loss: 329.8148 - val_mae: 12.7514\n",
            "Epoch 21/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 212.7825 - mae: 10.9589 - val_loss: 326.3617 - val_mae: 12.6709\n",
            "Epoch 22/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 203.2167 - mae: 10.7174 - val_loss: 321.3147 - val_mae: 12.5928\n",
            "Epoch 23/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 192.6534 - mae: 10.4354 - val_loss: 317.1621 - val_mae: 12.5016\n",
            "Epoch 24/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 181.8930 - mae: 10.1284 - val_loss: 313.4911 - val_mae: 12.4298\n",
            "Epoch 25/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 171.3514 - mae: 9.8194 - val_loss: 309.5584 - val_mae: 12.3605\n",
            "Epoch 26/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 161.4063 - mae: 9.5315 - val_loss: 306.7201 - val_mae: 12.3143\n",
            "Epoch 27/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 152.1112 - mae: 9.2463 - val_loss: 305.3828 - val_mae: 12.2945\n",
            "Epoch 28/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 143.5051 - mae: 8.9845 - val_loss: 302.4337 - val_mae: 12.2737\n",
            "Epoch 29/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 135.5154 - mae: 8.7239 - val_loss: 301.4457 - val_mae: 12.2724\n",
            "Epoch 30/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 128.0958 - mae: 8.4769 - val_loss: 301.7323 - val_mae: 12.2869\n",
            "Epoch 31/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 121.1820 - mae: 8.2446 - val_loss: 300.1534 - val_mae: 12.2809\n",
            "Epoch 32/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 114.8508 - mae: 8.0203 - val_loss: 299.6510 - val_mae: 12.2864\n",
            "Epoch 33/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 108.8692 - mae: 7.8080 - val_loss: 299.7209 - val_mae: 12.3046\n",
            "Epoch 34/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 103.3199 - mae: 7.6065 - val_loss: 297.9393 - val_mae: 12.2949\n",
            "Epoch 35/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 98.2076 - mae: 7.3909 - val_loss: 299.6150 - val_mae: 12.3374\n",
            "Epoch 36/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 93.6060 - mae: 7.2171 - val_loss: 297.0204 - val_mae: 12.3160\n",
            "Epoch 37/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 89.1217 - mae: 7.0301 - val_loss: 296.4911 - val_mae: 12.3168\n",
            "Epoch 38/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 84.9967 - mae: 6.8466 - val_loss: 298.0252 - val_mae: 12.3567\n",
            "Epoch 39/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 81.0164 - mae: 6.6755 - val_loss: 297.6539 - val_mae: 12.3690\n",
            "Epoch 40/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 77.3707 - mae: 6.5120 - val_loss: 297.8683 - val_mae: 12.3908\n",
            "Epoch 41/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 73.9963 - mae: 6.3549 - val_loss: 297.8821 - val_mae: 12.4057\n",
            "Epoch 42/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 70.7540 - mae: 6.2095 - val_loss: 298.9933 - val_mae: 12.4365\n",
            "Epoch 43/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 67.6446 - mae: 6.0572 - val_loss: 300.6691 - val_mae: 12.4644\n",
            "Epoch 44/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 64.7976 - mae: 5.9160 - val_loss: 300.6851 - val_mae: 12.4827\n",
            "Epoch 45/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 62.0202 - mae: 5.7755 - val_loss: 302.6453 - val_mae: 12.5281\n",
            "Epoch 46/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 59.5011 - mae: 5.6513 - val_loss: 302.6324 - val_mae: 12.5482\n",
            "Epoch 47/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 57.0065 - mae: 5.5237 - val_loss: 303.8043 - val_mae: 12.5845\n",
            "Epoch 48/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 54.8094 - mae: 5.4157 - val_loss: 303.1598 - val_mae: 12.5864\n",
            "Epoch 49/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 52.6142 - mae: 5.2871 - val_loss: 304.8795 - val_mae: 12.6309\n",
            "Epoch 50/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 50.3565 - mae: 5.1656 - val_loss: 304.7295 - val_mae: 12.6430\n",
            "Epoch 51/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 48.3892 - mae: 5.0507 - val_loss: 305.3975 - val_mae: 12.6674\n",
            "Epoch 52/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 46.4829 - mae: 4.9487 - val_loss: 307.0354 - val_mae: 12.7115\n",
            "Epoch 53/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 44.6214 - mae: 4.8390 - val_loss: 305.3910 - val_mae: 12.6978\n",
            "Epoch 54/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 42.9316 - mae: 4.7261 - val_loss: 306.1557 - val_mae: 12.7319\n",
            "Epoch 55/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 41.2932 - mae: 4.6274 - val_loss: 306.4666 - val_mae: 12.7559\n",
            "Epoch 56/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 39.6500 - mae: 4.5226 - val_loss: 308.3150 - val_mae: 12.7958\n",
            "Epoch 57/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 38.1444 - mae: 4.4301 - val_loss: 309.2947 - val_mae: 12.8308\n",
            "Epoch 58/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 36.6976 - mae: 4.3319 - val_loss: 308.1842 - val_mae: 12.8294\n",
            "Epoch 59/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 35.3014 - mae: 4.2342 - val_loss: 307.8508 - val_mae: 12.8364\n",
            "Epoch 60/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 33.9927 - mae: 4.1521 - val_loss: 310.4418 - val_mae: 12.8942\n",
            "Epoch 61/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 32.7431 - mae: 4.0550 - val_loss: 309.4515 - val_mae: 12.8879\n",
            "Epoch 62/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 31.5493 - mae: 3.9702 - val_loss: 310.1548 - val_mae: 12.9161\n",
            "Epoch 63/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 30.2150 - mae: 3.8618 - val_loss: 312.6225 - val_mae: 12.9718\n",
            "Epoch 64/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 29.1722 - mae: 3.7874 - val_loss: 311.8209 - val_mae: 12.9688\n",
            "Epoch 65/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 28.1357 - mae: 3.7171 - val_loss: 311.4472 - val_mae: 12.9737\n",
            "Epoch 66/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 27.0569 - mae: 3.6265 - val_loss: 312.0690 - val_mae: 12.9949\n",
            "Epoch 67/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 26.0455 - mae: 3.5430 - val_loss: 312.3230 - val_mae: 13.0147\n",
            "Epoch 68/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 25.0945 - mae: 3.4661 - val_loss: 312.8408 - val_mae: 13.0279\n",
            "Epoch 69/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 24.1652 - mae: 3.3944 - val_loss: 312.2949 - val_mae: 13.0266\n",
            "Epoch 70/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 23.3352 - mae: 3.3255 - val_loss: 314.1169 - val_mae: 13.0695\n",
            "Epoch 71/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 22.4472 - mae: 3.2515 - val_loss: 314.8817 - val_mae: 13.0854\n",
            "Epoch 72/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 21.5840 - mae: 3.1736 - val_loss: 314.6256 - val_mae: 13.0905\n",
            "Epoch 73/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 20.7732 - mae: 3.0929 - val_loss: 315.2728 - val_mae: 13.1110\n",
            "Epoch 74/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 20.1282 - mae: 3.0382 - val_loss: 314.3628 - val_mae: 13.0975\n",
            "Epoch 75/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 19.3541 - mae: 2.9669 - val_loss: 316.1834 - val_mae: 13.1385\n",
            "Epoch 76/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 18.6506 - mae: 2.8969 - val_loss: 315.1974 - val_mae: 13.1402\n",
            "Epoch 77/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 18.0009 - mae: 2.8356 - val_loss: 314.7380 - val_mae: 13.1335\n",
            "Epoch 78/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 17.3700 - mae: 2.7761 - val_loss: 316.3351 - val_mae: 13.1653\n",
            "Epoch 79/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 16.6944 - mae: 2.7028 - val_loss: 317.0176 - val_mae: 13.1818\n",
            "Epoch 80/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 16.0758 - mae: 2.6432 - val_loss: 317.4971 - val_mae: 13.2020\n",
            "Epoch 81/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 15.5004 - mae: 2.5791 - val_loss: 317.7249 - val_mae: 13.2070\n",
            "Epoch 82/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 14.9864 - mae: 2.5251 - val_loss: 316.1413 - val_mae: 13.1933\n",
            "Epoch 83/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 14.4061 - mae: 2.4591 - val_loss: 317.8618 - val_mae: 13.2252\n",
            "Epoch 84/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 13.9011 - mae: 2.4044 - val_loss: 319.4005 - val_mae: 13.2662\n",
            "Epoch 85/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 13.4022 - mae: 2.3476 - val_loss: 319.7313 - val_mae: 13.2727\n",
            "Epoch 86/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 12.9029 - mae: 2.2854 - val_loss: 318.8906 - val_mae: 13.2703\n",
            "Epoch 87/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 12.4967 - mae: 2.2307 - val_loss: 320.5757 - val_mae: 13.3047\n",
            "Epoch 88/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 12.0462 - mae: 2.1832 - val_loss: 319.7189 - val_mae: 13.2984\n",
            "Epoch 89/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 11.6099 - mae: 2.1271 - val_loss: 320.6439 - val_mae: 13.3214\n",
            "Epoch 90/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 11.2579 - mae: 2.0782 - val_loss: 319.6982 - val_mae: 13.3151\n",
            "Epoch 91/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 10.8516 - mae: 2.0293 - val_loss: 320.0603 - val_mae: 13.3280\n",
            "Epoch 92/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 10.4983 - mae: 1.9837 - val_loss: 322.2833 - val_mae: 13.3718\n",
            "Epoch 93/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 10.0889 - mae: 1.9277 - val_loss: 321.9081 - val_mae: 13.3709\n",
            "Epoch 94/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 9.7796 - mae: 1.8880 - val_loss: 321.0676 - val_mae: 13.3712\n",
            "Epoch 95/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 9.4540 - mae: 1.8396 - val_loss: 321.3257 - val_mae: 13.3789\n",
            "Epoch 96/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 9.1453 - mae: 1.7948 - val_loss: 321.7521 - val_mae: 13.3901\n",
            "Epoch 97/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 8.8487 - mae: 1.7576 - val_loss: 320.8267 - val_mae: 13.3829\n",
            "Epoch 98/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 8.5038 - mae: 1.6986 - val_loss: 322.0661 - val_mae: 13.4095\n",
            "Epoch 99/100\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 8.2273 - mae: 1.6616 - val_loss: 325.2159 - val_mae: 13.4652\n",
            "Epoch 100/100\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 8.0348 - mae: 1.6335 - val_loss: 321.4418 - val_mae: 13.4139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PNRpWVSDRDa",
        "outputId": "26a74afd-f5a1-4f0a-cdcd-e896e97dd614"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "neigh = KNeighborsRegressor(n_neighbors=2,metric='manhattan')\n",
        "neigh.fit(X1_train,y1_train)\n",
        "\n",
        "predictions = neigh.predict(X1_test)\n",
        "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y1_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 4.016014234875445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_pBDHyRDj-s",
        "outputId": "267f56e5-e793-4581-c3c7-1ce1907ee852"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Mean Squared Error: \" + str(mean_squared_error(predictions, y1_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 5.210409252669039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzagXF4pF-IH",
        "outputId": "34f62831-6acd-4a92-9fc6-2d615dc01556"
      },
      "source": [
        "from sklearn.metrics import max_error\n",
        "\n",
        "max_error(y1_test, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZDzbKnlRqFQ",
        "outputId": "d6e9566c-561b-4fdd-b284-7707d7221619"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "#gbrt1 = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt1 = GradientBoostingRegressor(random_state=0)\n",
        "gbrt1.fit(X1_train, y1_train)\n",
        "\n",
        "predictions1 = gbrt1.predict(X1_test)\n",
        "print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions1, y1_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 2.862513492129289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmsuLmU3S8xf",
        "outputId": "73bde196-ed13-4d2c-a4da-61edcfc6a59a"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Mean Squared Error: \" + str(mean_squared_error(predictions1, y1_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 13.620725035729514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNlaNYDeUbSK",
        "outputId": "e42af26d-4170-488e-d68e-90733767d41f"
      },
      "source": [
        "from sklearn.metrics import max_error\n",
        "\n",
        "max_error(y1_test, predictions1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.628715475840409"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrpYh3xlVkTQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=X1_train.shape[1:]))\n",
        "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(120, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(240, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(120, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3jSPJXN7K5Y",
        "outputId": "92b9fa72-a7a7-4a34-f639-8f3f6c4b4159"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 2961)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 30)                88860     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 120)               3720      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 240)               29040     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 120)               28920     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 30)                3630      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 154,201\n",
            "Trainable params: 154,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyfzfJgYYK-J"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
        "patience=50,\n",
        "restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcFmoZJcYx9P",
        "outputId": "caae0d3a-2908-412d-d637-410925387dc2"
      },
      "source": [
        "model.save(\"my_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsgJDSeZYhg6"
      },
      "source": [
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model\",\n",
        "save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBVWBJkGVvcw",
        "outputId": "35c51c31-49cf-4db1-fc44-5f0ae5b104ff"
      },
      "source": [
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam()\n",
        "#(clipnorm=1) , clipvalue=0.5\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=optimizer,metrics=['mae'])\n",
        "history = model.fit(X1_train, y1_train, epochs=500,\n",
        "validation_data=(X1_test, y1_test),\n",
        "callbacks=[checkpoint_cb, early_stopping_cb]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "53/53 [==============================] - 1s 11ms/step - loss: 3326.5354 - mae: 48.0372 - val_loss: 448.8700 - val_mae: 15.0767\n",
            "Epoch 2/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 266.8205 - mae: 12.4046 - val_loss: 266.9669 - val_mae: 12.6449\n",
            "Epoch 3/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 128.2651 - mae: 8.7023 - val_loss: 200.3173 - val_mae: 10.4987\n",
            "Epoch 4/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 59.3720 - mae: 5.8407 - val_loss: 172.9851 - val_mae: 9.9191\n",
            "Epoch 5/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 28.4638 - mae: 3.9666 - val_loss: 159.4067 - val_mae: 9.5383\n",
            "Epoch 6/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 14.9173 - mae: 2.8036 - val_loss: 149.7839 - val_mae: 9.3234\n",
            "Epoch 7/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 8.9157 - mae: 2.0869 - val_loss: 149.4923 - val_mae: 9.2652\n",
            "Epoch 8/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 5.9381 - mae: 1.5835 - val_loss: 141.2022 - val_mae: 9.0334\n",
            "Epoch 9/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 4.2449 - mae: 1.2837 - val_loss: 137.5641 - val_mae: 9.0145\n",
            "Epoch 10/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 3.2488 - mae: 1.0385 - val_loss: 134.0838 - val_mae: 8.8249\n",
            "Epoch 11/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.6463 - mae: 0.9060 - val_loss: 131.6247 - val_mae: 8.7920\n",
            "Epoch 12/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.7098 - mae: 0.9354 - val_loss: 127.5295 - val_mae: 8.6144\n",
            "Epoch 13/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.9372 - mae: 0.9982 - val_loss: 127.1975 - val_mae: 8.6615\n",
            "Epoch 14/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 3.0417 - mae: 1.0335 - val_loss: 121.8627 - val_mae: 8.4702\n",
            "Epoch 15/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 3.1695 - mae: 1.1031 - val_loss: 120.2568 - val_mae: 8.4206\n",
            "Epoch 16/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.8829 - mae: 0.9935 - val_loss: 118.6689 - val_mae: 8.2805\n",
            "Epoch 17/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.5774 - mae: 0.9269 - val_loss: 116.2045 - val_mae: 8.2713\n",
            "Epoch 18/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.3898 - mae: 0.9114 - val_loss: 114.4849 - val_mae: 8.1784\n",
            "Epoch 19/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.0599 - mae: 0.8388 - val_loss: 111.8252 - val_mae: 8.1159\n",
            "Epoch 20/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.8428 - mae: 0.7903 - val_loss: 109.7335 - val_mae: 7.9421\n",
            "Epoch 21/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.6576 - mae: 0.7399 - val_loss: 107.1503 - val_mae: 7.9405\n",
            "Epoch 22/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.5550 - mae: 0.6839 - val_loss: 106.1912 - val_mae: 7.8128\n",
            "Epoch 23/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.8394 - mae: 0.7756 - val_loss: 103.0019 - val_mae: 7.7767\n",
            "Epoch 24/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.9349 - mae: 0.8315 - val_loss: 102.5087 - val_mae: 7.6905\n",
            "Epoch 25/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.6794 - mae: 0.7739 - val_loss: 99.0912 - val_mae: 7.5773\n",
            "Epoch 26/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.5530 - mae: 0.7597 - val_loss: 97.8706 - val_mae: 7.4892\n",
            "Epoch 27/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.6343 - mae: 0.7944 - val_loss: 92.9298 - val_mae: 7.3786\n",
            "Epoch 28/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.8481 - mae: 0.8682 - val_loss: 93.0337 - val_mae: 7.3244\n",
            "Epoch 29/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.9572 - mae: 0.9120 - val_loss: 89.2467 - val_mae: 7.2096\n",
            "Epoch 30/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.3999 - mae: 1.0471 - val_loss: 87.6643 - val_mae: 7.0664\n",
            "Epoch 31/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.7113 - mae: 1.0966 - val_loss: 83.1352 - val_mae: 6.9173\n",
            "Epoch 32/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.5976 - mae: 1.0627 - val_loss: 80.7638 - val_mae: 6.7947\n",
            "Epoch 33/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.1059 - mae: 0.9238 - val_loss: 77.1139 - val_mae: 6.6166\n",
            "Epoch 34/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.8036 - mae: 0.8394 - val_loss: 78.0468 - val_mae: 6.6310\n",
            "Epoch 35/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.4845 - mae: 0.7883 - val_loss: 74.4998 - val_mae: 6.4719\n",
            "Epoch 36/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.4002 - mae: 0.7960 - val_loss: 72.9492 - val_mae: 6.4286\n",
            "Epoch 37/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.2099 - mae: 0.7285 - val_loss: 69.3342 - val_mae: 6.3218\n",
            "Epoch 38/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.3103 - mae: 0.7895 - val_loss: 70.1855 - val_mae: 6.2929\n",
            "Epoch 39/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.2446 - mae: 0.7646 - val_loss: 66.1450 - val_mae: 6.1886\n",
            "Epoch 40/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.0487 - mae: 0.6988 - val_loss: 65.3470 - val_mae: 6.0912\n",
            "Epoch 41/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.0654 - mae: 0.6845 - val_loss: 63.4234 - val_mae: 6.0298\n",
            "Epoch 42/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.0858 - mae: 0.7049 - val_loss: 63.4358 - val_mae: 6.0311\n",
            "Epoch 43/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.1752 - mae: 1.0066 - val_loss: 62.1848 - val_mae: 5.9302\n",
            "Epoch 44/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.2553 - mae: 1.0874 - val_loss: 58.0552 - val_mae: 5.8228\n",
            "Epoch 45/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.3447 - mae: 1.0365 - val_loss: 57.4806 - val_mae: 5.7230\n",
            "Epoch 46/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.0703 - mae: 0.9461 - val_loss: 53.8155 - val_mae: 5.5224\n",
            "Epoch 47/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.4635 - mae: 0.8641 - val_loss: 56.2323 - val_mae: 5.5576\n",
            "Epoch 48/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.8474 - mae: 0.9727 - val_loss: 52.0906 - val_mae: 5.4273\n",
            "Epoch 49/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.4613 - mae: 0.8630 - val_loss: 49.6945 - val_mae: 5.3073\n",
            "Epoch 50/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.9054 - mae: 0.6968 - val_loss: 50.4627 - val_mae: 5.3375\n",
            "Epoch 51/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6973 - mae: 0.6025 - val_loss: 48.1888 - val_mae: 5.2686\n",
            "Epoch 52/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6103 - mae: 0.5848 - val_loss: 48.1519 - val_mae: 5.2541\n",
            "Epoch 53/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5721 - mae: 0.5814 - val_loss: 47.8067 - val_mae: 5.2110\n",
            "Epoch 54/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4473 - mae: 0.4993 - val_loss: 46.8711 - val_mae: 5.1883\n",
            "Epoch 55/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3840 - mae: 0.4622 - val_loss: 47.0636 - val_mae: 5.1787\n",
            "Epoch 56/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2969 - mae: 0.3986 - val_loss: 46.2803 - val_mae: 5.1476\n",
            "Epoch 57/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3166 - mae: 0.4257 - val_loss: 45.6662 - val_mae: 5.0770\n",
            "Epoch 58/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3764 - mae: 0.4653 - val_loss: 45.7201 - val_mae: 5.1023\n",
            "Epoch 59/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4466 - mae: 0.4983 - val_loss: 45.7346 - val_mae: 5.0720\n",
            "Epoch 60/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.5317 - mae: 0.5306 - val_loss: 44.9144 - val_mae: 5.0730\n",
            "Epoch 61/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8060 - mae: 0.6852 - val_loss: 44.3693 - val_mae: 5.0343\n",
            "Epoch 62/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9719 - mae: 0.7340 - val_loss: 43.5852 - val_mae: 5.0470\n",
            "Epoch 63/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.1430 - mae: 0.7931 - val_loss: 43.3349 - val_mae: 4.9308\n",
            "Epoch 64/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.1574 - mae: 0.7998 - val_loss: 42.5580 - val_mae: 4.8931\n",
            "Epoch 65/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.6205 - mae: 0.9777 - val_loss: 41.7135 - val_mae: 4.8850\n",
            "Epoch 66/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.0047 - mae: 1.0829 - val_loss: 37.4057 - val_mae: 4.6824\n",
            "Epoch 67/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.5947 - mae: 0.9642 - val_loss: 38.3430 - val_mae: 4.6248\n",
            "Epoch 68/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.3695 - mae: 0.8828 - val_loss: 35.6123 - val_mae: 4.5517\n",
            "Epoch 69/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.0610 - mae: 0.7744 - val_loss: 36.4259 - val_mae: 4.4964\n",
            "Epoch 70/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9040 - mae: 0.7158 - val_loss: 34.1419 - val_mae: 4.4243\n",
            "Epoch 71/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8966 - mae: 0.7082 - val_loss: 34.3859 - val_mae: 4.4536\n",
            "Epoch 72/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.7554 - mae: 0.6435 - val_loss: 34.5145 - val_mae: 4.4321\n",
            "Epoch 73/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6443 - mae: 0.5965 - val_loss: 33.6194 - val_mae: 4.3695\n",
            "Epoch 74/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6423 - mae: 0.6060 - val_loss: 35.3175 - val_mae: 4.4491\n",
            "Epoch 75/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6493 - mae: 0.6084 - val_loss: 32.9357 - val_mae: 4.3562\n",
            "Epoch 76/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6233 - mae: 0.5851 - val_loss: 33.6143 - val_mae: 4.4015\n",
            "Epoch 77/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6339 - mae: 0.5719 - val_loss: 33.4657 - val_mae: 4.3361\n",
            "Epoch 78/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6011 - mae: 0.5529 - val_loss: 32.8092 - val_mae: 4.3709\n",
            "Epoch 79/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6272 - mae: 0.5679 - val_loss: 32.8165 - val_mae: 4.3024\n",
            "Epoch 80/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6365 - mae: 0.5774 - val_loss: 32.6787 - val_mae: 4.3175\n",
            "Epoch 81/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7662 - mae: 0.6325 - val_loss: 31.9690 - val_mae: 4.2626\n",
            "Epoch 82/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.8805 - mae: 0.6791 - val_loss: 31.3649 - val_mae: 4.2923\n",
            "Epoch 83/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.3717 - mae: 0.8940 - val_loss: 31.0781 - val_mae: 4.2145\n",
            "Epoch 84/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.1859 - mae: 1.0630 - val_loss: 30.8914 - val_mae: 4.1267\n",
            "Epoch 85/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.9584 - mae: 0.9978 - val_loss: 30.4778 - val_mae: 4.2219\n",
            "Epoch 86/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.2786 - mae: 1.1671 - val_loss: 28.1173 - val_mae: 4.0611\n",
            "Epoch 87/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.1646 - mae: 1.0469 - val_loss: 27.7329 - val_mae: 3.9884\n",
            "Epoch 88/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 2.4214 - mae: 1.1577 - val_loss: 26.8088 - val_mae: 3.9126\n",
            "Epoch 89/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.6107 - mae: 0.9166 - val_loss: 25.0495 - val_mae: 3.8365\n",
            "Epoch 90/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.1559 - mae: 0.7263 - val_loss: 25.1706 - val_mae: 3.7876\n",
            "Epoch 91/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.7010 - mae: 0.6102 - val_loss: 24.9518 - val_mae: 3.7849\n",
            "Epoch 92/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6175 - mae: 0.5533 - val_loss: 26.3651 - val_mae: 3.9605\n",
            "Epoch 93/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5353 - mae: 0.5597 - val_loss: 25.4262 - val_mae: 3.8157\n",
            "Epoch 94/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3474 - mae: 0.4330 - val_loss: 25.4565 - val_mae: 3.8394\n",
            "Epoch 95/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2385 - mae: 0.3619 - val_loss: 25.5299 - val_mae: 3.8180\n",
            "Epoch 96/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2098 - mae: 0.3367 - val_loss: 25.7758 - val_mae: 3.8223\n",
            "Epoch 97/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2510 - mae: 0.3936 - val_loss: 25.3087 - val_mae: 3.8381\n",
            "Epoch 98/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2586 - mae: 0.3949 - val_loss: 25.0249 - val_mae: 3.8130\n",
            "Epoch 99/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2283 - mae: 0.3614 - val_loss: 25.3642 - val_mae: 3.8023\n",
            "Epoch 100/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1939 - mae: 0.3258 - val_loss: 25.0169 - val_mae: 3.7871\n",
            "Epoch 101/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2863 - mae: 0.3970 - val_loss: 25.4106 - val_mae: 3.7989\n",
            "Epoch 102/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.5272 - mae: 0.5661 - val_loss: 26.8675 - val_mae: 3.8776\n",
            "Epoch 103/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9186 - mae: 0.7213 - val_loss: 24.3728 - val_mae: 3.7665\n",
            "Epoch 104/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.0587 - mae: 0.7778 - val_loss: 25.6493 - val_mae: 3.8024\n",
            "Epoch 105/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.7591 - mae: 1.0501 - val_loss: 26.8507 - val_mae: 3.9124\n",
            "Epoch 106/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 2.2573 - mae: 1.1828 - val_loss: 22.9580 - val_mae: 3.7390\n",
            "Epoch 107/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.4518 - mae: 0.9136 - val_loss: 21.3481 - val_mae: 3.4826\n",
            "Epoch 108/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.8684 - mae: 0.7246 - val_loss: 21.8075 - val_mae: 3.5676\n",
            "Epoch 109/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7019 - mae: 0.6555 - val_loss: 21.5704 - val_mae: 3.5549\n",
            "Epoch 110/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8212 - mae: 0.7259 - val_loss: 21.3300 - val_mae: 3.4923\n",
            "Epoch 111/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.7956 - mae: 0.6908 - val_loss: 21.8658 - val_mae: 3.5170\n",
            "Epoch 112/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.9722 - mae: 0.7641 - val_loss: 20.9451 - val_mae: 3.4813\n",
            "Epoch 113/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.0782 - mae: 0.8098 - val_loss: 20.8452 - val_mae: 3.4493\n",
            "Epoch 114/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.9269 - mae: 0.7458 - val_loss: 20.1688 - val_mae: 3.4231\n",
            "Epoch 115/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9763 - mae: 0.7613 - val_loss: 20.8862 - val_mae: 3.5226\n",
            "Epoch 116/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8812 - mae: 0.7080 - val_loss: 19.6624 - val_mae: 3.3727\n",
            "Epoch 117/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6669 - mae: 0.6006 - val_loss: 19.1458 - val_mae: 3.3016\n",
            "Epoch 118/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4638 - mae: 0.4915 - val_loss: 19.7184 - val_mae: 3.3691\n",
            "Epoch 119/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2955 - mae: 0.3842 - val_loss: 19.6425 - val_mae: 3.3202\n",
            "Epoch 120/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2136 - mae: 0.3268 - val_loss: 19.6069 - val_mae: 3.3652\n",
            "Epoch 121/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1652 - mae: 0.2766 - val_loss: 19.3705 - val_mae: 3.2995\n",
            "Epoch 122/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2132 - mae: 0.3391 - val_loss: 19.6653 - val_mae: 3.3602\n",
            "Epoch 123/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2880 - mae: 0.4032 - val_loss: 19.8381 - val_mae: 3.3908\n",
            "Epoch 124/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2902 - mae: 0.3982 - val_loss: 19.1326 - val_mae: 3.2999\n",
            "Epoch 125/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2957 - mae: 0.4076 - val_loss: 19.1584 - val_mae: 3.3194\n",
            "Epoch 126/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2614 - mae: 0.3675 - val_loss: 19.6339 - val_mae: 3.3187\n",
            "Epoch 127/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4047 - mae: 0.4903 - val_loss: 18.8877 - val_mae: 3.2836\n",
            "Epoch 128/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.8218 - mae: 0.6982 - val_loss: 18.6831 - val_mae: 3.2773\n",
            "Epoch 129/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8895 - mae: 0.7256 - val_loss: 19.9542 - val_mae: 3.4700\n",
            "Epoch 130/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9877 - mae: 0.7606 - val_loss: 18.0754 - val_mae: 3.2207\n",
            "Epoch 131/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.8304 - mae: 0.6844 - val_loss: 17.7858 - val_mae: 3.2159\n",
            "Epoch 132/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6242 - mae: 0.5755 - val_loss: 18.2496 - val_mae: 3.2440\n",
            "Epoch 133/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5720 - mae: 0.5686 - val_loss: 17.8954 - val_mae: 3.1947\n",
            "Epoch 134/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7408 - mae: 0.5503 - val_loss: 17.8799 - val_mae: 3.1722\n",
            "Epoch 135/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.0703 - mae: 0.8154 - val_loss: 20.9162 - val_mae: 3.4321\n",
            "Epoch 136/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.2605 - mae: 0.8749 - val_loss: 17.2562 - val_mae: 3.1385\n",
            "Epoch 137/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9563 - mae: 0.7500 - val_loss: 18.1034 - val_mae: 3.2025\n",
            "Epoch 138/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.9585 - mae: 0.7555 - val_loss: 17.3981 - val_mae: 3.1624\n",
            "Epoch 139/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9563 - mae: 0.7164 - val_loss: 16.8889 - val_mae: 3.1122\n",
            "Epoch 140/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5874 - mae: 0.5808 - val_loss: 17.4361 - val_mae: 3.2050\n",
            "Epoch 141/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5433 - mae: 0.5717 - val_loss: 17.2276 - val_mae: 3.1914\n",
            "Epoch 142/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4231 - mae: 0.4952 - val_loss: 17.1518 - val_mae: 3.1746\n",
            "Epoch 143/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3105 - mae: 0.4266 - val_loss: 17.2312 - val_mae: 3.1882\n",
            "Epoch 144/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4421 - mae: 0.5201 - val_loss: 16.7650 - val_mae: 3.1151\n",
            "Epoch 145/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6364 - mae: 0.5845 - val_loss: 16.2796 - val_mae: 3.0592\n",
            "Epoch 146/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.4023 - mae: 0.9509 - val_loss: 16.6312 - val_mae: 3.1059\n",
            "Epoch 147/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.1195 - mae: 0.8040 - val_loss: 17.3280 - val_mae: 3.1195\n",
            "Epoch 148/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.0378 - mae: 0.8036 - val_loss: 16.0068 - val_mae: 3.0573\n",
            "Epoch 149/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9666 - mae: 0.7446 - val_loss: 16.9777 - val_mae: 3.1778\n",
            "Epoch 150/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.0818 - mae: 0.8245 - val_loss: 16.0931 - val_mae: 3.0740\n",
            "Epoch 151/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 1.4386 - mae: 0.9528 - val_loss: 16.6434 - val_mae: 3.1856\n",
            "Epoch 152/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 1.1979 - mae: 0.8377 - val_loss: 15.6769 - val_mae: 2.9822\n",
            "Epoch 153/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7806 - mae: 0.6696 - val_loss: 15.6325 - val_mae: 3.0443\n",
            "Epoch 154/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6700 - mae: 0.6257 - val_loss: 16.3949 - val_mae: 3.0407\n",
            "Epoch 155/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7038 - mae: 0.6351 - val_loss: 16.2472 - val_mae: 3.1146\n",
            "Epoch 156/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6476 - mae: 0.5876 - val_loss: 15.8084 - val_mae: 3.0341\n",
            "Epoch 157/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.7160 - mae: 0.6332 - val_loss: 15.7105 - val_mae: 3.0402\n",
            "Epoch 158/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5624 - mae: 0.5464 - val_loss: 15.6654 - val_mae: 3.0386\n",
            "Epoch 159/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5947 - mae: 0.5261 - val_loss: 15.1102 - val_mae: 2.9740\n",
            "Epoch 160/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4099 - mae: 0.4858 - val_loss: 15.8816 - val_mae: 3.0051\n",
            "Epoch 161/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3322 - mae: 0.4393 - val_loss: 15.6051 - val_mae: 3.0332\n",
            "Epoch 162/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3366 - mae: 0.4367 - val_loss: 16.2896 - val_mae: 3.0996\n",
            "Epoch 163/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3124 - mae: 0.4138 - val_loss: 15.8384 - val_mae: 3.0308\n",
            "Epoch 164/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3216 - mae: 0.4209 - val_loss: 15.5698 - val_mae: 3.0158\n",
            "Epoch 165/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4064 - mae: 0.4897 - val_loss: 15.5849 - val_mae: 3.0215\n",
            "Epoch 166/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4751 - mae: 0.5079 - val_loss: 15.5015 - val_mae: 3.0048\n",
            "Epoch 167/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3653 - mae: 0.4578 - val_loss: 15.5665 - val_mae: 3.0268\n",
            "Epoch 168/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3724 - mae: 0.4608 - val_loss: 15.2130 - val_mae: 2.9654\n",
            "Epoch 169/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3733 - mae: 0.4629 - val_loss: 15.2554 - val_mae: 2.9634\n",
            "Epoch 170/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3353 - mae: 0.4361 - val_loss: 15.2001 - val_mae: 2.9558\n",
            "Epoch 171/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3165 - mae: 0.4128 - val_loss: 14.9960 - val_mae: 2.9211\n",
            "Epoch 172/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3261 - mae: 0.4307 - val_loss: 15.1164 - val_mae: 2.9569\n",
            "Epoch 173/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2917 - mae: 0.3963 - val_loss: 15.5910 - val_mae: 3.0342\n",
            "Epoch 174/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5132 - mae: 0.5643 - val_loss: 15.7985 - val_mae: 3.0357\n",
            "Epoch 175/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6683 - mae: 0.6226 - val_loss: 15.0544 - val_mae: 2.9710\n",
            "Epoch 176/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7841 - mae: 0.6668 - val_loss: 15.1810 - val_mae: 2.9778\n",
            "Epoch 177/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6159 - mae: 0.5916 - val_loss: 14.9135 - val_mae: 2.9496\n",
            "Epoch 178/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4011 - mae: 0.4624 - val_loss: 15.2786 - val_mae: 3.0049\n",
            "Epoch 179/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3032 - mae: 0.3917 - val_loss: 14.7447 - val_mae: 2.9195\n",
            "Epoch 180/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3375 - mae: 0.4311 - val_loss: 14.8353 - val_mae: 2.9488\n",
            "Epoch 181/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4007 - mae: 0.4795 - val_loss: 14.8804 - val_mae: 2.9394\n",
            "Epoch 182/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.5972 - mae: 0.5999 - val_loss: 15.0196 - val_mae: 2.9748\n",
            "Epoch 183/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6882 - mae: 0.6435 - val_loss: 14.8788 - val_mae: 2.9230\n",
            "Epoch 184/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4845 - mae: 0.5258 - val_loss: 15.2709 - val_mae: 2.9833\n",
            "Epoch 185/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4678 - mae: 0.5200 - val_loss: 14.8019 - val_mae: 2.9259\n",
            "Epoch 186/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5345 - mae: 0.5629 - val_loss: 14.9682 - val_mae: 2.9666\n",
            "Epoch 187/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5369 - mae: 0.5569 - val_loss: 15.0335 - val_mae: 2.9290\n",
            "Epoch 188/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4537 - mae: 0.5064 - val_loss: 14.7960 - val_mae: 2.9066\n",
            "Epoch 189/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2765 - mae: 0.3881 - val_loss: 15.2600 - val_mae: 3.0045\n",
            "Epoch 190/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3634 - mae: 0.4672 - val_loss: 15.9476 - val_mae: 3.0968\n",
            "Epoch 191/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3645 - mae: 0.4468 - val_loss: 14.4410 - val_mae: 2.8757\n",
            "Epoch 192/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6468 - mae: 0.6426 - val_loss: 14.3290 - val_mae: 2.8628\n",
            "Epoch 193/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7739 - mae: 0.6820 - val_loss: 14.7844 - val_mae: 2.9319\n",
            "Epoch 194/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6501 - mae: 0.6063 - val_loss: 14.2905 - val_mae: 2.8917\n",
            "Epoch 195/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6601 - mae: 0.6105 - val_loss: 14.5170 - val_mae: 2.9327\n",
            "Epoch 196/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7522 - mae: 0.6584 - val_loss: 14.1522 - val_mae: 2.8426\n",
            "Epoch 197/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8647 - mae: 0.7343 - val_loss: 14.8554 - val_mae: 2.9139\n",
            "Epoch 198/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.7589 - mae: 0.6512 - val_loss: 14.3515 - val_mae: 2.8672\n",
            "Epoch 199/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.5161 - mae: 0.5356 - val_loss: 14.6682 - val_mae: 2.9287\n",
            "Epoch 200/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3317 - mae: 0.4250 - val_loss: 14.7500 - val_mae: 2.9483\n",
            "Epoch 201/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2248 - mae: 0.3427 - val_loss: 14.2081 - val_mae: 2.8574\n",
            "Epoch 202/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2458 - mae: 0.3672 - val_loss: 14.2644 - val_mae: 2.8656\n",
            "Epoch 203/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4409 - mae: 0.4789 - val_loss: 14.3872 - val_mae: 2.8805\n",
            "Epoch 204/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3638 - mae: 0.4479 - val_loss: 14.1739 - val_mae: 2.8728\n",
            "Epoch 205/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5074 - mae: 0.4359 - val_loss: 14.6962 - val_mae: 2.9306\n",
            "Epoch 206/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3373 - mae: 0.4194 - val_loss: 14.1361 - val_mae: 2.8690\n",
            "Epoch 207/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.5654 - mae: 0.5866 - val_loss: 14.3842 - val_mae: 2.8631\n",
            "Epoch 208/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8286 - mae: 0.7066 - val_loss: 15.0825 - val_mae: 2.9726\n",
            "Epoch 209/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6122 - mae: 0.5799 - val_loss: 14.5007 - val_mae: 2.9408\n",
            "Epoch 210/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4366 - mae: 0.4908 - val_loss: 14.0798 - val_mae: 2.8245\n",
            "Epoch 211/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4818 - mae: 0.5327 - val_loss: 14.3520 - val_mae: 2.9046\n",
            "Epoch 212/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4448 - mae: 0.4894 - val_loss: 13.9147 - val_mae: 2.8303\n",
            "Epoch 213/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3736 - mae: 0.4523 - val_loss: 14.2146 - val_mae: 2.8870\n",
            "Epoch 214/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2509 - mae: 0.3616 - val_loss: 13.9778 - val_mae: 2.8505\n",
            "Epoch 215/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1813 - mae: 0.3194 - val_loss: 14.2175 - val_mae: 2.8538\n",
            "Epoch 216/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1528 - mae: 0.2911 - val_loss: 14.1630 - val_mae: 2.8658\n",
            "Epoch 217/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1624 - mae: 0.3005 - val_loss: 14.0194 - val_mae: 2.8533\n",
            "Epoch 218/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1285 - mae: 0.2678 - val_loss: 14.0703 - val_mae: 2.8482\n",
            "Epoch 219/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1722 - mae: 0.3249 - val_loss: 14.2518 - val_mae: 2.8804\n",
            "Epoch 220/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1631 - mae: 0.3057 - val_loss: 14.3477 - val_mae: 2.9019\n",
            "Epoch 221/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1602 - mae: 0.3008 - val_loss: 14.3389 - val_mae: 2.8913\n",
            "Epoch 222/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1454 - mae: 0.2758 - val_loss: 14.0347 - val_mae: 2.8579\n",
            "Epoch 223/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1382 - mae: 0.2754 - val_loss: 14.2660 - val_mae: 2.8909\n",
            "Epoch 224/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1367 - mae: 0.2693 - val_loss: 14.1079 - val_mae: 2.8533\n",
            "Epoch 225/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2168 - mae: 0.3506 - val_loss: 14.2130 - val_mae: 2.8498\n",
            "Epoch 226/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3955 - mae: 0.4920 - val_loss: 14.1420 - val_mae: 2.8808\n",
            "Epoch 227/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4401 - mae: 0.4982 - val_loss: 14.0511 - val_mae: 2.8282\n",
            "Epoch 228/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6075 - mae: 0.5991 - val_loss: 14.8124 - val_mae: 2.8958\n",
            "Epoch 229/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9698 - mae: 0.7779 - val_loss: 14.1891 - val_mae: 2.8342\n",
            "Epoch 230/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.9483 - mae: 0.7502 - val_loss: 13.9484 - val_mae: 2.8590\n",
            "Epoch 231/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6666 - mae: 0.6186 - val_loss: 13.8396 - val_mae: 2.8251\n",
            "Epoch 232/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.5110 - mae: 0.5371 - val_loss: 13.9651 - val_mae: 2.8375\n",
            "Epoch 233/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4653 - mae: 0.5292 - val_loss: 13.8805 - val_mae: 2.8518\n",
            "Epoch 234/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3050 - mae: 0.4089 - val_loss: 14.4222 - val_mae: 2.9116\n",
            "Epoch 235/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2313 - mae: 0.3612 - val_loss: 14.0234 - val_mae: 2.8466\n",
            "Epoch 236/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2464 - mae: 0.3920 - val_loss: 14.2019 - val_mae: 2.9078\n",
            "Epoch 237/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2008 - mae: 0.3438 - val_loss: 14.5533 - val_mae: 2.9312\n",
            "Epoch 238/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2131 - mae: 0.3576 - val_loss: 13.9630 - val_mae: 2.8472\n",
            "Epoch 239/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2295 - mae: 0.3634 - val_loss: 13.9662 - val_mae: 2.8491\n",
            "Epoch 240/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1775 - mae: 0.3116 - val_loss: 13.9495 - val_mae: 2.8428\n",
            "Epoch 241/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2060 - mae: 0.3469 - val_loss: 14.0397 - val_mae: 2.8551\n",
            "Epoch 242/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2504 - mae: 0.3833 - val_loss: 14.7616 - val_mae: 2.9777\n",
            "Epoch 243/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3189 - mae: 0.4316 - val_loss: 14.0758 - val_mae: 2.8704\n",
            "Epoch 244/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2306 - mae: 0.3568 - val_loss: 14.0121 - val_mae: 2.8379\n",
            "Epoch 245/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1774 - mae: 0.3158 - val_loss: 13.9278 - val_mae: 2.8480\n",
            "Epoch 246/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2096 - mae: 0.3460 - val_loss: 13.7621 - val_mae: 2.8090\n",
            "Epoch 247/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2484 - mae: 0.3863 - val_loss: 13.8872 - val_mae: 2.8402\n",
            "Epoch 248/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1946 - mae: 0.3311 - val_loss: 13.9463 - val_mae: 2.8425\n",
            "Epoch 249/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2472 - mae: 0.3864 - val_loss: 13.9398 - val_mae: 2.8491\n",
            "Epoch 250/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4950 - mae: 0.5623 - val_loss: 13.9084 - val_mae: 2.8471\n",
            "Epoch 251/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4475 - mae: 0.4993 - val_loss: 14.7161 - val_mae: 2.8811\n",
            "Epoch 252/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3988 - mae: 0.4799 - val_loss: 14.4554 - val_mae: 2.9313\n",
            "Epoch 253/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4451 - mae: 0.5234 - val_loss: 14.2175 - val_mae: 2.8301\n",
            "Epoch 254/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.7765 - mae: 0.6998 - val_loss: 14.2173 - val_mae: 2.8653\n",
            "Epoch 255/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.8424 - mae: 0.7057 - val_loss: 15.5285 - val_mae: 3.0546\n",
            "Epoch 256/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6402 - mae: 0.6098 - val_loss: 14.0070 - val_mae: 2.8201\n",
            "Epoch 257/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6202 - mae: 0.6113 - val_loss: 14.4601 - val_mae: 2.9185\n",
            "Epoch 258/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3794 - mae: 0.4534 - val_loss: 14.1447 - val_mae: 2.8787\n",
            "Epoch 259/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2230 - mae: 0.3536 - val_loss: 14.0877 - val_mae: 2.8576\n",
            "Epoch 260/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1309 - mae: 0.2635 - val_loss: 14.2286 - val_mae: 2.8841\n",
            "Epoch 261/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1117 - mae: 0.2618 - val_loss: 14.2980 - val_mae: 2.8926\n",
            "Epoch 262/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1191 - mae: 0.2686 - val_loss: 14.4074 - val_mae: 2.9182\n",
            "Epoch 263/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1965 - mae: 0.3527 - val_loss: 14.1621 - val_mae: 2.8709\n",
            "Epoch 264/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1232 - mae: 0.2626 - val_loss: 14.2012 - val_mae: 2.8690\n",
            "Epoch 265/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0915 - mae: 0.2197 - val_loss: 14.0031 - val_mae: 2.8358\n",
            "Epoch 266/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0825 - mae: 0.2173 - val_loss: 14.0748 - val_mae: 2.8414\n",
            "Epoch 267/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0619 - mae: 0.1816 - val_loss: 13.9600 - val_mae: 2.8381\n",
            "Epoch 268/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0515 - mae: 0.1613 - val_loss: 14.1230 - val_mae: 2.8642\n",
            "Epoch 269/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0405 - mae: 0.1385 - val_loss: 14.1355 - val_mae: 2.8701\n",
            "Epoch 270/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0569 - mae: 0.1712 - val_loss: 14.0868 - val_mae: 2.8571\n",
            "Epoch 271/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0740 - mae: 0.2039 - val_loss: 14.0325 - val_mae: 2.8576\n",
            "Epoch 272/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1190 - mae: 0.2668 - val_loss: 14.1634 - val_mae: 2.8348\n",
            "Epoch 273/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1404 - mae: 0.2845 - val_loss: 14.4193 - val_mae: 2.9131\n",
            "Epoch 274/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1839 - mae: 0.3257 - val_loss: 14.7104 - val_mae: 2.9464\n",
            "Epoch 275/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2324 - mae: 0.3705 - val_loss: 14.8221 - val_mae: 2.9625\n",
            "Epoch 276/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4566 - mae: 0.5182 - val_loss: 14.6794 - val_mae: 2.9337\n",
            "Epoch 277/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.5824 - mae: 0.5872 - val_loss: 13.4753 - val_mae: 2.7707\n",
            "Epoch 278/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6527 - mae: 0.6249 - val_loss: 14.5125 - val_mae: 2.9408\n",
            "Epoch 279/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3897 - mae: 0.4665 - val_loss: 14.3660 - val_mae: 2.8651\n",
            "Epoch 280/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2474 - mae: 0.3650 - val_loss: 13.8304 - val_mae: 2.8456\n",
            "Epoch 281/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2731 - mae: 0.3999 - val_loss: 14.0492 - val_mae: 2.8443\n",
            "Epoch 282/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3160 - mae: 0.4384 - val_loss: 14.2582 - val_mae: 2.8762\n",
            "Epoch 283/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2251 - mae: 0.3536 - val_loss: 14.5393 - val_mae: 2.8873\n",
            "Epoch 284/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2150 - mae: 0.3531 - val_loss: 13.8768 - val_mae: 2.8125\n",
            "Epoch 285/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2002 - mae: 0.3441 - val_loss: 13.9501 - val_mae: 2.8366\n",
            "Epoch 286/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2214 - mae: 0.3617 - val_loss: 14.3755 - val_mae: 2.8703\n",
            "Epoch 287/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2962 - mae: 0.4208 - val_loss: 14.1869 - val_mae: 2.8600\n",
            "Epoch 288/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3016 - mae: 0.4192 - val_loss: 14.2182 - val_mae: 2.8645\n",
            "Epoch 289/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3937 - mae: 0.4972 - val_loss: 13.9002 - val_mae: 2.8461\n",
            "Epoch 290/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.3326 - mae: 0.4409 - val_loss: 14.3922 - val_mae: 2.8996\n",
            "Epoch 291/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2236 - mae: 0.3630 - val_loss: 14.1870 - val_mae: 2.8414\n",
            "Epoch 292/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1672 - mae: 0.3027 - val_loss: 14.1860 - val_mae: 2.8744\n",
            "Epoch 293/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1370 - mae: 0.2744 - val_loss: 14.2339 - val_mae: 2.8810\n",
            "Epoch 294/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0954 - mae: 0.2246 - val_loss: 14.4112 - val_mae: 2.8980\n",
            "Epoch 295/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0752 - mae: 0.2045 - val_loss: 14.1491 - val_mae: 2.8698\n",
            "Epoch 296/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0716 - mae: 0.1985 - val_loss: 14.2187 - val_mae: 2.8781\n",
            "Epoch 297/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1005 - mae: 0.2474 - val_loss: 14.0948 - val_mae: 2.8518\n",
            "Epoch 298/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1501 - mae: 0.3040 - val_loss: 14.4115 - val_mae: 2.9193\n",
            "Epoch 299/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1451 - mae: 0.2874 - val_loss: 13.9921 - val_mae: 2.8253\n",
            "Epoch 300/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2207 - mae: 0.3666 - val_loss: 14.7550 - val_mae: 2.9518\n",
            "Epoch 301/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1657 - mae: 0.3041 - val_loss: 13.7898 - val_mae: 2.8216\n",
            "Epoch 302/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1434 - mae: 0.2824 - val_loss: 14.1457 - val_mae: 2.8695\n",
            "Epoch 303/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1237 - mae: 0.2591 - val_loss: 14.1265 - val_mae: 2.8521\n",
            "Epoch 304/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1372 - mae: 0.2791 - val_loss: 13.9475 - val_mae: 2.8337\n",
            "Epoch 305/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2371 - mae: 0.3624 - val_loss: 13.9963 - val_mae: 2.8637\n",
            "Epoch 306/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3174 - mae: 0.4389 - val_loss: 14.1704 - val_mae: 2.8732\n",
            "Epoch 307/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.4157 - mae: 0.4951 - val_loss: 14.1991 - val_mae: 2.8883\n",
            "Epoch 308/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.6044 - mae: 0.6149 - val_loss: 15.0680 - val_mae: 2.9204\n",
            "Epoch 309/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6482 - mae: 0.6160 - val_loss: 14.4945 - val_mae: 2.9030\n",
            "Epoch 310/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.6452 - mae: 0.6241 - val_loss: 15.2271 - val_mae: 3.0096\n",
            "Epoch 311/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.7068 - mae: 0.6618 - val_loss: 14.1880 - val_mae: 2.8738\n",
            "Epoch 312/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3752 - mae: 0.4666 - val_loss: 14.3697 - val_mae: 2.8840\n",
            "Epoch 313/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.2181 - mae: 0.3530 - val_loss: 14.2603 - val_mae: 2.9054\n",
            "Epoch 314/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1861 - mae: 0.3244 - val_loss: 14.3054 - val_mae: 2.8592\n",
            "Epoch 315/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.2416 - mae: 0.3873 - val_loss: 14.3083 - val_mae: 2.8662\n",
            "Epoch 316/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1724 - mae: 0.3172 - val_loss: 14.4402 - val_mae: 2.9307\n",
            "Epoch 317/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.1636 - mae: 0.3130 - val_loss: 14.3251 - val_mae: 2.8929\n",
            "Epoch 318/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1060 - mae: 0.2398 - val_loss: 14.1187 - val_mae: 2.8547\n",
            "Epoch 319/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1380 - mae: 0.2940 - val_loss: 14.5536 - val_mae: 2.9410\n",
            "Epoch 320/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.1187 - mae: 0.2626 - val_loss: 14.0494 - val_mae: 2.8667\n",
            "Epoch 321/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0806 - mae: 0.2098 - val_loss: 14.2586 - val_mae: 2.8897\n",
            "Epoch 322/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0593 - mae: 0.1741 - val_loss: 14.2333 - val_mae: 2.8752\n",
            "Epoch 323/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0402 - mae: 0.1389 - val_loss: 14.1208 - val_mae: 2.8636\n",
            "Epoch 324/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0255 - mae: 0.1107 - val_loss: 14.1546 - val_mae: 2.8603\n",
            "Epoch 325/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0329 - mae: 0.1370 - val_loss: 14.2676 - val_mae: 2.8901\n",
            "Epoch 326/500\n",
            "53/53 [==============================] - 0s 5ms/step - loss: 0.0561 - mae: 0.1833 - val_loss: 14.1278 - val_mae: 2.8515\n",
            "Epoch 327/500\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.0902 - mae: 0.2386 - val_loss: 14.2615 - val_mae: 2.8725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53qYhXIw_Jaz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}